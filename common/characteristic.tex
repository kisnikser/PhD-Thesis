{\actuality} В статистической теории обучения~\cite{vapnik1995nature} обоснована принципиальная связь между сложностью семейства моделей, объемом обучающей выборки и способностью к обобщению: для надежного выбора модели требуется достаточное количество данных. Подавляющее большинство современных моделей машинного обучения являются параметрическими: они задаются конечным набором параметров, определяемых минимизацией эмпирической функции потерь на выборке. Поскольку функция потерь вычисляется по ограниченным данным и зависит от параметров модели, возникает фундаментальный вопрос о взаимосвязи объема выборки, структуры модели и свойств множества оптимальных параметров, а также о минимальном объеме данных, необходимом для успешного обучения~\cite{kaplan2020scaling,hoffmann2022chinchila}.

Эмпирические законы масштабирования показывают, что качество моделей закономерно зависит от размера данных и модели: в языковых моделях установлены степенные зависимости потерь от объема данных и числа параметров~\cite{kaplan2020scaling}, а для вычисленно-оптимального обучения предложено согласованное масштабирование размера модели и объема данных~\cite{hoffmann2022chinchila}. Эти результаты опираются на масштабные эксперименты и не имеют пока единого теоретического обоснования, связывающего геометрию оптимизационной задачи с объемом выборки.

Ключевой объект для такого обоснования "--- ландшафт функции потерь в пространстве параметров. Его визуализация и анализ структуры минимумов~\cite{li2018visualizing}, изучение локальной геометрии и связей с обобщающей способностью~\cite{fort2019large}, а также доказательство связности множества решений~\cite{draxler2018essentially} показывают, что геометрия поверхности функции потерь определяет как выбор решения методами градиентной оптимизации~\cite{garipov2018loss,chaudhari2019entropy}, так и устойчивость к возмущениям данных. Спектральные свойства матрицы Гессе в окрестности минимума характеризуют кривизну поверхности и используются при анализе сходимости и обобщения~\cite{papyan2019spectrum}. Вместе с тем остается открытой задача систематического описания того, как ландшафт функции потерь стабилизируется при увеличении объема выборки и как связать эту стабилизацию с законами масштабирования и достаточным размером данных.

Таким образом, актуальной задачей является разработка методов, связывающих три компонента: параметрическую модель машинного обучения, объем обучающей выборки (в предположении независимости и одинаковой распределенности наблюдений) и геометрию множества оптимальных параметров, задаваемую ландшафтом функции потерь, с опорой на фундаментальные результаты теории обучения и современные эмпирические законы масштабирования.

{\aim} данной работы является разработка математических методов описания законов масштабирования параметрических моделей машинного обучения (от классических линейных моделей до глубоких нейронных сетей) на основе анализа оптимизационной поверхности. Для достижения цели поставлены и решены следующие {\tasks}:
\begin{enumerate}[beginpenalty=10000]
    \item Разработать метод оценки законов масштабирования вероятностной модели линейной регрессии на основе бутстрапирования функции правдоподобия по подвыборкам; получить теоретические оценки связи увеличения размера обучающей выборки со стабилизацией математического ожидания и дисперсии функции правдоподобия.
    \item Разработать метод оценки законов масштабирования вероятностной модели линейной регрессии на основе близости апостериорных распределений параметров на подвыборках; получить теоретические оценки связи увеличения числа обучающих примеров со стабилизацией апостериорного распределения параметров модели.
    \item Разработать метод оценки законов масштабирования матрично-представимых нейронных сетей на основе стабилизации поверхности функции потерь при увеличении размера выборки; предложить и математически обосновать численный метод сравнения значений функции потерь в точке ее минимума при добавлении новых данных.
    \item Разработать численный метод оценки сходимости поверхности функции потерь в нейронных сетях при увеличении объема выборки на основе сэмплирования точек в окрестности минимума; предложить и обосновать переход к подпространству меньшей размерности для повышения вычислительной эффективности и улучшения оценок скорости сходимости.
\end{enumerate}

{\novelty}
\begin{enumerate}[beginpenalty=10000]
    \item Впервые предложен метод оценки законов масштабирования вероятностной модели линейной регрессии на основе бутстрапирования функции правдоподобия по подвыборкам; метод не требует построения полной кривой обучения и снабжен теоретическими оценками стабилизации математического ожидания и дисперсии функции правдоподобия при увеличении объема данных.
    \item Впервые предложен метод оценки законов масштабирования вероятностной модели линейной регрессии на основе близости апостериорных распределений параметров на подвыборках; в качестве мер близости используются KL-дивергенция и функция s-score, что позволяет количественно характеризовать сходимость модели без многократного полного переобучения.
    \item Впервые предложен метод оценки законов масштабирования матрично-представимых нейронных сетей на основе сходимости ландшафта функции потерь при увеличении размера выборки; в основе подхода "--- разложение Гаусса "--~Ньютона и спектральный анализ матрицы Гессе. Метод дает теоретическое обоснование известных эмпирических законов масштабирования~\cite{hoffmann2022chinchila}.
    \item Впервые предложен численный метод оценки сходимости ландшафта функции потерь в нейронных сетях на основе сэмплирования по Монте "--~Карло в окрестности минимума с переходом к подпространству, натянутому на главные собственные векторы матрицы Гессе; метод сочетает вычислительную эффективность с повышением точности оценок скорости сходимости.
\end{enumerate}

{\tinfluence} полученных результатов состоит в том, что получено теоретическое обоснование связи между объемом обучающей выборки и стабилизацией оптимизационной поверхности (для линейных моделей "--- через правдоподобие и апостериорные распределения, для нейронных сетей "--- через спектр матрицы Гессе и сходимость ландшафта функции потерь), что дополняет известные эмпирические законы масштабирования строгими оценками и единой постановкой задачи.

{\pinfluence} настоящей работы заключается в возможности применения разработанных методов для обоснованного определения достаточного объема данных при планировании экспериментов и сборе выборок для линейных и нейросетевых моделей, а также для формулировки критериев остановки сбора данных и интерпретации достаточности выборки в терминах геометрии поверхности функции потерь.

{\methods} Для достижения поставленных
в диссертационной работе целей используются:
\begin{enumerate}[beginpenalty=10000]
    \item Методы линейной алгебры, математического и функционального анализа: матричное дифференцирование, анализ функций многих переменных, спектральный анализ матриц (в том числе матрицы Гессе).
    \item Методы теории вероятностей и математической статистики: байесовский вывод, анализ сходимости последовательностей случайных величин и распределений, элементы теории оптимизации.
    \item Методы вычислительного эксперимента и анализа результатов: бутстрап, сэмплирование по Монте "--~Карло, сравнение с базовыми методами оценки размера выборки, воспроизводимая постановка экспериментов.
\end{enumerate}

{\defpositions}
\begin{enumerate}[beginpenalty=10000]
  \item Метод оценки законов масштабирования вероятностной модели линейной регрессии на основе бутстрапирования функции правдоподобия по подвыборкам с теоретическими оценками скорости стабилизации математического ожидания и дисперсии функции правдоподобия; получены верхние границы зависимости достаточного объема выборки от заданной точности (критерии D- и M-достаточности).
  \item Метод оценки законов масштабирования вероятностной модели линейной регрессии на основе сходимости KL-дивергенции и функции близости s-score между апостериорными распределениями на схожих подвыборках; получены верхние границы зависимости достаточного объема выборки от заданной близости распределений (критерии KL- и S-достаточности).
  \item Метод оценки законов масштабирования матрично-представимых нейронных сетей на основе разложения Гаусса "--~Ньютона и спектрального анализа матрицы Гессе; получены оценки спектральной нормы матрицы Гессе для полносвязных, сверточных и трансформерных архитектур и связь сходимости поверхности потерь с объемом выборки.
  \item Численный метод оценки сходимости ландшафта функции потерь в нейронных сетях на основе сэмплирования по Монте "--~Карло в окрестности минимума и проекции на подпространство главных собственных векторов матрицы Гессе; метод обеспечивает вычислительно эффективную оценку скорости стабилизации поверхности при увеличении объема данных.
\end{enumerate}

{\reliability} полученных результатов обеспечивается корректностью
применения апробированного в научной практике математического аппарата теории вероятностей, математической статистики и других смежных теоретических областей, а также экспериментальной проверкой разработанных методов на большом количестве модельных и практических задач машинного обучения. Полученные теоретические результы обосновываются математически строгими доказательствами, а для проведенных вычислительных экспериментов даются детальные описания, обеспечивающие их воспроизводимость. Результаты опубликованы в рецензируемых научных изданиях и в трудах рецензируемых российских и международных конференций по машинному обучению и искусственному интеллекту.

{\probation} Основные результаты работы докладывались и обсуждались на следующих научных конференциях:
\begin{enumerate}
    \item Открытая конференция ИСП РАН, Москва, 9--10 декабря 2025~г.
    \item 22-я Всероссийская конференция с международным участием
    <<Математические методы распознавания образов (ММРО-2025)>>,
    Муром, 22--26 сентября 2025~г.
    \item 67-я Всероссийская научная конференция МФТИ,
    Москва, 31 марта -- 5 апреля 2025~г.
    \item Открытая конференция ИСП РАН, Москва, 11--12 декабря 2024~г.
    \item 66-я Всероссийская научная конференция МФТИ,
    Москва, 1--6 апреля 2024~г.
\end{enumerate}


{\contribution} Все приведенные результаты, кроме отдельно оговоренных случаев, получены диссертантом лично при научном руководстве к.ф.-м.н.~А.\,В.~Грабового.

\ifnumequal{\value{bibliosel}}{0}
{%%% Встроенная реализация с загрузкой файла через движок bibtex8. (При желании, внутри можно использовать обычные ссылки, наподобие `\cite{vakbib1,vakbib2}`).
    {\publications} Основные результаты по теме диссертации изложены
    в~XX~печатных изданиях,
    X из которых изданы в журналах, рекомендованных ВАК,
    X "--- в тезисах докладов.
}%
{%%% Реализация пакетом biblatex через движок biber
    \begin{refsection}[bl-author, bl-registered]
        % Это refsection=1.
        % Процитированные здесь работы:
        %  * подсчитываются, для автоматического составления фразы "Основные результаты ..."
        %  * попадают в авторскую библиографию, при usefootcite==0 и стиле `\insertbiblioauthor` или `\insertbiblioauthorgrouped`
        %  * нумеруются там в зависимости от порядка команд `\printbibliography` в этом разделе.
        %  * при использовании `\insertbiblioauthorgrouped`, порядок команд `\printbibliography` в нем должен быть тем же (см. biblio/biblatex.tex)
        %
        % Невидимый библиографический список для подсчета количества публикаций:
        \phantom{\printbibliography[heading=nobibheading, section=1, env=countauthorvak,          keyword=biblioauthorvak]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorwos,          keyword=biblioauthorwos]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorscopus,       keyword=biblioauthorscopus]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorconf,         keyword=biblioauthorconf]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorother,        keyword=biblioauthorother]%
        \printbibliography[heading=nobibheading, section=1, env=countauthor,             keyword=biblioauthor]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorvakscopuswos, filter=vakscopuswos]%
        \printbibliography[heading=nobibheading, section=1, env=countauthorscopuswos,    filter=scopuswos]}%
        %
        \nocite{*}%
        %
        % {\publications} Основные результаты по теме диссертации изложены в~\arabic{citeauthor}~печатных изданиях~\cite{},
        % \arabic{citeauthorvak} из которых изданы в журналах, рекомендованных ВАК%
        % \ifnum \value{citeauthorscopuswos}>0%
        %     , \arabic{citeauthorscopuswos} "--- в~периодических научных журналах, индексируемых Web of~Science и Scopus%
        % \fi%
        % \ifnum \value{citeauthorconf}>0%
        %     , \arabic{citeauthorconf} "--- в~тезисах докладов.
        % \else%
        %     .
        % \fi%
        % \ifnum \value{citeregistered}=1%
        %     \ifnum \value{citeauthorpatent}=1%
        %         Зарегистрирован \arabic{citeauthorpatent} патент.
        %     \fi%
        %     \ifnum \value{citeauthorprogram}=1%
        %         Зарегистрирована \arabic{citeauthorprogram} программа для ЭВМ.
        %     \fi%
        % \fi%
        % \ifnum \value{citeregistered}>1%
        %     Зарегистрированы\ %
        %     \ifnum \value{citeauthorpatent}>0%
        %     \formbytotal{citeauthorpatent}{патент}{}{а}{}%
        %     \ifnum \value{citeauthorprogram}=0 . \else \ и~\fi%
        %     \fi%
        %     \ifnum \value{citeauthorprogram}>0%
        %     \formbytotal{citeauthorprogram}{программ}{а}{ы}{} для ЭВМ.
        %     \fi%
        % \fi%
        {\publications} \ifsynopsis\textit{Список публикаций приведен в конце автореферата. }\else\textit{Список публикаций приведен в конце диссертации. }\fi Основные результаты по теме диссертации изложены в~\arabic{citeauthor}~научных работах, из которых~\arabic{citeauthorscopus} "--- в~периодических научных журналах, индексируемых Scopus и Web of~Science, а остальные~\arabic{citeauthorconf} "--- в~тезисах докладов российских и международных конференций.
        % К публикациям, в которых излагаются основные научные результаты диссертации на соискание ученой
        % степени, в рецензируемых изданиях приравниваются патенты на изобретения, патенты (свидетельства) на
        % полезную модель, патенты на промышленный образец, патенты на селекционные достижения, свидетельства
        % на программу для электронных вычислительных машин, базу данных, топологию интегральных микросхем,
        % зарегистрированные в установленном порядке.(в ред. Постановления Правительства РФ от 21.04.2016 N 335)
    \end{refsection}%
    \begin{refsection}[bl-author, bl-registered]
        % Это refsection=2.
        % Процитированные здесь работы:
        %  * попадают в авторскую библиографию, при usefootcite==0 и стиле `\insertbiblioauthorimportant`.
        %  * ни на что не влияют в противном случае
        % \nocite{kiselev2025ssdlb}
        % \nocite{kiselev2025ssdpdp}
        % \nocite{kiselev2024unraveling}
        % \nocite{kiselev2024conf66mipt}
        % \nocite{meshkov2024convnets}
        % \nocite{kiselev2025conf67mipt}
        % \nocite{meshkov2025conf67mipt}
        % \nocite{kiselev2025mmro}
    \end{refsection}%
        %
        % Все, что вне этих двух refsection, это refsection=0,
        %  * для диссертации - это нормальные ссылки, попадающие в обычную библиографию
        %  * для автореферата:
        %     * при usefootcite==0, ссылка корректно сработает только для источника из `external.bib`. Для своих работ --- напечатает "[0]" (и даже Warning не вылезет).
        %     * при usefootcite==1, ссылка сработает нормально. В авторской библиографии будут только процитированные в refsection=0 работы.
}