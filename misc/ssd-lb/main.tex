% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
\input{preamble}
%
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}
\urlstyle{rm}
%
\begin{document}
%
\title{Достаточный размер выборки: бутстрапирование правдоподобия}
%
\titlerunning{Достаточный размер выборки: бутстрапирование правдоподобия}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{Nikita Kiselev\inst{1} \and Andrey Grabovoy\inst{1}}
\author{Киселев Никита Сергеевич\inst{1}, Грабовой Андрей Валериевич\inst{2}}
%
\authorrunning{Киселев Никита Сергеевич, Грабовой Андрей Валериевич}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Московский физико-технический институт, Москва, Россия;\\
$^1$\texttt{kiselev.ns@phystech.edu};\\
$^2$\texttt{grabovoy.av@phystech.edu}.
}
%
\maketitle % typeset the header of the contribution
%
\begin{abstract}

Определение подходящего размера выборки имеет решающее значение для построения эффективных моделей машинного обучения. Существующие методы часто либо не имеют строгого теоретического обоснования, либо привязаны к конкретным статистическим гипотезам о параметрах модели. В настоящей работе представляются два новых метода, основанных на значениях правдоподобия на бутстрапированных подвыборках. Демонстрируется корректность одного из этих методов на в модели линейной регрессии. Вычислительные эксперименты как с синтетическими, так и с реальными наборами данных показывают, что предложенные функции сходятся по мере увеличения размера выборки, что подчеркивает практическую полезность нашего подхода.

\textbf{Ключевые слова:} Достаточный размер выборки, Бутстрапирование правдоподобия, Линейная регрессия, Вычислительная линейная алгебра
\end{abstract}
%
%
%
\section{Введение}
Задача машинного обучения с учителем предполагает выбор предсказательной модели из некоторого параметрического семейства. Обычно такой выбор связан с некоторыми статистическими гипотезами, например, максимизацией некоторого функционала качества. Модель, которая соответствует этим статистическим гипотезам, называется \textit{адекватной} моделью \cite{bies2006genetic,cawley2010over}.

При планировании вычислительного эксперимента требуется оценить минимальный размер выборки~--- количество объектов, необходимое для построения адекватной модели. Размер выборки, необходимый для построения адекватной модели прогнозирования, называется \textit{достаточным} \cite{byrd2012sample,figueroa2012predicting,balki2019sample}. 

В работе рассматривается проблема определения достаточного размера выборки. Этой теме посвящено большое число работ. Используемые в них подходы можно разделить на статистические, байесовские и эвристические.

Одни из первых исследований по данной теме \cite{Adcock1988,Joseph1995} формулируют определенный статистический критерий, где связанный с данным критерием метод оценки размера выборки гарантирует достижение фиксированной статистической мощности с величиной ошибки первого рода, не превышающей заданного значения. К статистическим методам относятся метод множителей Лагранжа \cite{self1988power}, метод проверки отношения правдоподобия \cite{shieh2000power}, метод Вальда \cite{shieh2005power}. Статистические методы имеют ряд ограничений, которые связаны с их применением на практике. Они позволяют оценить размер выборки, исходя из предположений о распределении данных и информации о соответствии наблюдаемых величин предположениям нулевой гипотезы.

Байесовский подход тоже имеет место в данной проблеме. В работе \cite{Lindley1997} достаточный размер выборки определяется исходя из максимизации ожидаемой функции полезности. Она может включать в себя в явном виде функции распределения параметров и штрафы за увеличение размера выборки. Также в этой работе рассматриваются альтернативные подходы, основанные на ограничении некоторого критерия качества оценки параметров модели. Среди таких критериев можно выделить критерий средней апостериорной дисперсии (APVC), критерий среднего покрытия (ACC), критерий средней длины (ALC) и критерий эффективного объема выборки (ESC). Эти критерии получили свое развитие в других работах, например, \cite{PhamGia1997} и \cite{Gelfand2002}. Спустя время, авторы \cite{Cao2009} провели теоретическое и практическое сравнение методов из \cite{Adcock1988,Joseph1995,Lindley1997}.

Авторы \cite{Brutti2014}, как и \cite{Pezeshk2008}, рассматривают различия между байесовским и частотным подходами при определении размера выборки. Также они предлагают робастные методы для байесовского подхода и приводят наглядные примеры для некоторых вероятностных моделей.

В работе \cite{Grabovoy2022} рассматриваются различные методы оценки размера выборки в обобщенных линейных моделях, включая статистические, эвристические и байесовские методы. Анализируются такие методы, как тест на множители Лагранжа, тест на отношение правдоподобия, статистика Вальда, кросс-валидация, бутстрап, критерий Кульбака-Лейблера, критерий средней апостериорной дисперсии, критерий среднего охвата, критерий средней длины и максимизация полезности. Авторы указывают на возможное развитие темы, которое заключается в поиске метода, сочетающего байесовский и статистический подходы для оценки размера выборки для недостаточного доступного размера выборки.

В \cite{MOTRENKO2014743} рассматривается метод определения размера выборки в логистической регрессии, использующий кросс-валидацию и дивергенцию Кульбака-Лейблера между апостериорными распределениями параметров модели на схожих подвыборках. Под схожими подвыборками понимают такие подвыборки, которые могут быть получены друг из друга добавлением, удалением или заменой одного объекта.

В настоящей работе рассматриваются несколько подходов к определению достаточного размера выборки. Предлагается оценивать математическое ожидание и дисперсию функции правдоподобия на бутстрапированных подвыборках. Малое изменение этих величин при добавлении очередного объекта свидетельствует о достижении достаточного числа объектов в выборке. Доказывается корректность определения в модели линейной регрессии. Представленный метод легко использовать и на практике. Для этого предлагается подсчитывать значение функции ошибки, а не правдоподобия. 

\section{Постановка задачи}
Объектом называется пара $(\bx, y)$, где $\bx \in \mathbb{X} \subseteq \mathbb{R}^n$ есть вектор признакового описания объекта, а $y \in \mathbb{Y}$ есть значение целевой переменной. В задаче регрессии $\mathbb{Y} = \mathbb{R}$, а в задаче $K$-классовой классификации $\mathbb{Y} = \{1, \ldots, K\}$.

Матрицей объекты-признаки для выборки $\mathfrak{D}_m = \left\{ (\bx_i, y_i) \right\}, i \in \mathcal{I} = \{ 1, \ldots, m \}$ размера $m$ называется матрица $\bX_m = \left[ \bx_1, \ldots, \bx_m \right]\T \in \mathbb{R}^{m \times n}$.

Вектором ответов (вектором значений целевой переменной) для выборки $\mathfrak{D}_m = \left\{ (\bx_i, y_i) \right\}, i \in \mathcal{I} = \{ 1, \ldots, m \}$ размера $m$ называется вектор $\by_m = \left[ y_1, \ldots, y_m \right]\T \in \mathbb{Y}^m$.

Моделью называется параметрическое семейство функций $f$, отображающих декартово произведение множества значений признакового описания объектов $\mathbb{X}$ и множества значений параметров $\mathbb{W}$ во множество значений целевой переменной $\mathbb{Y}$: 
\[ f: \mathbb{X} \times \mathbb{W} \to \mathbb{Y}. \]

Вероятностной моделью называется совместное распределение вида 
\[ p(y, \bw | \bx) = p(y | \bx, \bw) p(\bw): \mathbb{Y} \times \mathbb{W} \times \mathbb{X} \to \mathbb{R}^+, \]
где $\bw \in \mathbb{W}$ есть набор параметров модели, $p(y | \bx, \bw)$ задает правдоподобие объекта, а $p(\bw)$ задает априорное распределение параметров.

Функцией правдоподобия простой выборки $\mathfrak{D}_m = \left\{ (\bx_i, y_i) \right\}, i \in \mathcal{I} = \{ 1, \ldots, m \}$ размера $m$ называется функция 
\[ L(\mathfrak{D}_m, \mathbf{w}) = p(\by_m | \bX_m, \bw) = \prod_{i=1}^{m} p(y_i | \mathbf{x}_i, \mathbf{w}). \]
Ее логарифм 
\[ l(\mathfrak{D}_m, \mathbf{w}) = \sum\limits_{i=1}^{m} \log p(y_i | \mathbf{x}_i, \mathbf{w}) \]
называется логарифмической функцией правдоподобия. Далее, если не оговорено противное, будем считать выборку простой.

Оценкой максимума правдоподобия набора параметров $\bw \in \mathbb{W}$ по подвыборке $\mathfrak{D}_k$ размера $k$ называется 
\[ \hat{\mathbf{w}}_{k} = \argmax_{\bw \in \mathbb{W}} L(\mathfrak{D}_k, \mathbf{w}). \]

Ставится задача определения достаточного размера выборки $m^*$. Пусть задан некоторый критерий $T$. Он может быть построен, например, на основе эвристик о поведении параметров модели.
\begin{definition}
    Размер выборки $m^*$ называется \textbf{достаточным} согласно критерию $T$, если $T$ выполняется для всех $k \geqslant m^*$.
\end{definition}

\section{Предлагаемые методы определения достаточного размера выборки}

В этом разделе будем считать, что достоверно $m^* \leqslant m$. Это означает, что нам нужно просто формализовать, какой размер выборки можно считать достаточным. Для определения достаточности будем использовать функцию правдоподобия. Когда в наличии имеется достаточно объектов, вполне естественно ожидать, что от одной реализации выборки к другой полученная оценка параметров не будет сильно меняться \cite{Joseph1997,Joseph1995}. То же можно сказать и про функцию правдоподобия. Таким образом, сформулируем, какой размер выборки можно считать достаточным..

\begin{definition}
    \label{sufficient-variance}
    Зафиксируем некоторое положительное число $\varepsilon > 0$. Размер выборки $m^*$ называется \textbf{D-достаточным}, если для всех $k \geqslant m^*$
    \[ D(k) = \mathbb{D}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \leqslant \varepsilon. \]
\end{definition}

С другой стороны, когда в наличии имеется достаточно объектов, также вполне естественно, что при добавлении очередного объекта в рассмотрение полученная оценка параметров не будет сильно меняться. Сформулируем еще одно определение.

\begin{definition}
    \label{sufficient-difference}
    Зафиксируем некоторое положительное число $\varepsilon > 0$. Размер выборки $m^*$ называется \textbf{M-достаточным}, если для всех $k \geqslant m^*$ 
    \[ M(k) = \left| \mathbb{E}_{\hat{\mathbf{w}}_{k+1}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\hat{\mathbf{w}}_{k}} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) \right| \leqslant \varepsilon. \]
\end{definition}
В определениях выше вместо функции правдоподобия $L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$ можно рассматривать ее логарифм $l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$.

Предположим, что $\mathbb{W} = \mathbb{R}^n$. Напомним, что информацией Фишера называется матрица
\[ \left[\mathcal{I}(\bw)\right]_{ij} = - \mathbb{E}\left[ \dfrac{\partial^2 \log p(\by | \bx, \bw)}{\partial w_i \partial w_j} \right]. \]
Известным результатом является асимптотическая нормальность оценки максимума правдоподобия, то есть $\sqrt{k}\left( \hat{\bw}_k - \bw \right) \xrightarrow{d} \mathcal{N}\left(0, \mathcal{I}^{-1}(\bw)\right)$. Из сходимости по распределению в общем случае не следует сходимость моментов случайного вектора. Тем не менее, если предположить последнее, то в некоторых моделях можно доказать корректность предложенного нами определения M-достаточного размера выборки.

Для удобства обозначим параметры распределения $\hat{\bw}_k$ следующим образом: математическое ожидание $\mathbb{E} \hat{\bw}_k = \bm_k$ и матрица ковариации $\mathbb{D} \hat{\bw}_k = \bSigma_k$. Тогда имеет место следующая теорема, доказательство которой приведено в Приложении.

\begin{theorem}\label{theorem}
    Пусть $\| \bm_{k+1} - \bm_k \|_2 \to 0$ и $\| \bSigma_{k+1} - \bSigma_k \|_{F} \to 0$ при $k \to \infty$. Тогда в модели линейной регрессии определение M-достаточного размера выборки является корректным. А именно, для любого $\varepsilon > 0$ найдется такой $m^*$, что для всех $k \geqslant m^*$ выполнено $M(k) \leqslant \varepsilon$.
\end{theorem}

\begin{corollary}\label{corollary}
    Пусть $\| \bm_k - \bw \|_2 \to 0$ и $\| \bSigma_k - \left[k\mathcal{I}(\bw)\right]^{-1} \|_{F} \to 0$ при $k \to \infty$. Тогда в модели линейной регрессии определение M-достаточного размера выборки является корректным. 
\end{corollary}

По условию задана одна выборка. Поэтому в эксперименте нет возможности посчитать указанные в определениях математическое ожидание и дисперсию. Для их оценки воспользуемся техникой бутстрап. А именно, сгенерируем из заданной $\mathfrak{D}_m$ некоторое число $B$ подвыборок размера $k$ с возвращением. Для каждой из них получим оценку параметров $\hat{\mathbf{w}}_{k}$ и посчитаем значение $L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k})$. Для оценки будем использовать выборочное среднее и несмещенную выборочную дисперсию (по бутстрап-выборкам).

Предложенные выше определения можно применять и в тех задачах, когда минимизируется произвольная функция потерь, а не максимизируется функция правдоподобия. Мы не приводим никаких теоретических обоснований этого, однако на практике такая эвристика оказывается достаточно удачной.

\section{Вычислительный эксперимент}

В данном разделе проводится эмпирическое исследование предлагаемых методов. Эксперименты проводятся на синтетических данных и на выборке Liver Disorders из библиотеки \cite{UCI}. Полностью воспроизводимый код экспериментов доступен в GitHub репозитории\footnote{\href{https://github.com/kisnikser/Likelihood-Bootstrapping}{\texttt{https://github.com/kisnikser/Likelihood-Bootstrapping}}}.

Синтетические данные сгенерированы из моделей линейной и логистической регрессий. Число объектов 1000, число признаков 20. Используется $B=1000$ бустрапированных подвыборок. Подсчитываются значения функций $D(k)$ и $M(k)$. Датасет с задачей регрессии Liver Disorders содержит 345 объектов и 5 признаков. Мы также используем $B=1000$ бутстрапированных подвыборок для оценки математического ожидания и дисперсии функции ошибки.

На Рис.~\ref{synthetic-regression} можно видеть полученные зависимости между используемым размером подвыборки $k$ и рассматриваемыми функциями $D(k)$ и $M(k)$ для синтетической выборки с задачей регрессии. Результаты для синтетической выборки с задачей классификации представлены на Рис.~\ref{synthetic-classification}. В то же время, на Рис.~\ref{liver-disorders} мы видим аналогичные графики для датасета Liver Disorders. Видно, что во всех случаях значения функций $D(k)$ и $M(k)$ стремятся к нулю при увеличении размера выборки. Эти эмпирические результаты подтверждают теоретические, полученные ранее.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{synthetic-regression}
    \caption{Сходимость предлагаемых функций $D(k)$ и $M(k)$ для выборки синтетической регрессии, то есть модели линейной регрессии. Обе функции стремятся к нулю при увеличении размера выборки.}
    \label{synthetic-regression}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{synthetic-classification}
    \caption{Сходимость предлагаемых функций $D(k)$ и $M(k)$ для выборки синтетической классификации, то есть модели логистической регрессии. Обе функции стремятся к нулю при увеличении размера выборки.}
    \label{synthetic-classification}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{liver-disorders}
    \caption{Сходимость предлагаемых функций $D(k)$ и $M(k)$ для выборки Liver Disorders. Обе функции стремятся к нулю при увеличении размера выборки.}
    \label{liver-disorders}
\end{figure}

В определениях D-достаточности и M-достаточности участвует гиперпараметр $\varepsilon$, который отвечает за порог для достаточного размера выборки $m^*$. С целью изучения зависимости между ними, мы представляем Рис.~\ref{sufficient-vs-threshold}, который демонстрирует, какой размер выборки следует выбрать, чтобы обеспечить определенный уровень уверенности.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{sufficient-vs-threshold}
    \caption{Зависимость достаточного размера выборки от значения порога на трех наборах данных: синтетическая регрессия, синтетическая классификация и Liver Disorders. При увеличении значения порога $\varepsilon$ достаточный размер уменьшается. Это означает, что можно выбирать меньше объектов для удовлетворения желаемых значений предлагаемых функций $D(k)$ и $M(k)$.}
    \label{sufficient-vs-threshold}
\end{figure}

Чтобы сравнить эффективность предложенных методов на разных наборах данных, были выбраны выборки из открытой библиотеки \cite{UCI}. Подробная информация о каждом наборе данных, количество наблюдений и количество признаков представлены в Таблице~\ref{table}. Для демонстрационных целей были выбраны такие значения гиперпараметра $\varepsilon$, при которых значения функций $D(k)$ и $M(k)$ уменьшаются в два раза. Соответствующие результаты приведены в Таблице~\ref{table}. Пропуски означают, что первоначальный размер выборки недостаточен.

\begin{table}[ht]
    \centering
    \caption{Сравнение предлагаемых методов определения достаточного размера выборки: на основе $D(k)$ и $M(k)$. Для каждой из предлагаемых функций подбирается такой значение порога, что ее изначальное значение уменьшается вдвое. Результаты представлены для набора выборок с задачей регрессии. Пропуски в данных означают, что исходный размер выборки недостаточен.}
    \label{table}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    Название & Объекты $m$ & Признаки $n$ & D & M \\
    \hline
    Abalone & 4177 & 8 & 96 & 96  \\
    Auto MPG & 392 & 8 & 15 & 15 \\
    Automobile & 159 & 25 & 70 & 156  \\
    Liver Disorders & 345 & 6 & 12 & 19  \\
    Servo & 167 & 4 & 41 & ---  \\
    Forest fires & 517 & 12 & 208 & --- \\
    Wine Quality & 6497 & 12 & 144 & 144  \\
    Energy Efficiency & 768 & 9 & 24 & 442  \\
    Student Performance & 649 & 32 & 129 & 177  \\
    Facebook Metrics & 495 & 18 & 31 & 388   \\
    Real Estate Valuation & 414 & 7 & 15 & 23  \\
    Heart Failure Clinical Records & 299 & 12 & 63 & 224  \\
    Bone marrow transplant: children & 142 & 36 & --- & --- \\
    \hline
    \end{tabular}
\end{table}

\clearpage
\section{Обсуждение}

В статье предлагаются два новых метода определения достаточного размера выборки, основанные на значениях правдоподобия на бутстрапированных подвыборках. Первый метод, называемый D-достаточностью, основан на дисперсии функции правдоподобия, в то время как второй, M-достаточность, фокусируется на разности в математическом ожидании функции правдоподобия при добавлении одного объекта в выборку. Демонстрируется корректность определения M-достаточности в модели линейной регрессии при определенных условиях на параметры модели.

Вычислительные эксперименты, проводенные как на синтетических, так и на реальных выборках, показывают, что предлагаемые функции $D(k)$ и $M(k)$ стремятся к нулю при увеличении размера выборки. Эксперименты также подчеркивают практическую значимость методов, поскольку они могут быть легко применены к различным наборам данных.

Предлагаемые методы потенциально могут быть применены к широкому спектру моделей и наборов данных, помимо линейной регрессии. Хотя мы доказали корректность определения M-достаточного размера выборки только для линейной регрессии, эмпирические результаты показывают, что эти методы могут быть эффективны и для других моделей. Будущая работа должна быть сосредоточена на распространении теоретического анализа на другие модели, включая, вероятно, нейронные сети.

\section{Заключение}

В статье представлены два новых метода, D-достаточность и M-достаточность, для определения достаточного размера выборки на основе значений правдоподобия на бутстрапированных подвыборках. Корректность определения M-достаточного размера выборки продемонстрирована в модели линейной регрессии, а вычислительные эксперименты на синтетических и реальных наборах данных показывают, что предложенные функции сходятся к нулю по мере увеличения размера выборки, что подчеркивает практичность методов. Будущая работа должна быть сосредоточена на распространении теоретического анализа на другие модели, включая нейронные сети.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\clearpage
\bibliographystyle{splncs04}
\bibliography{references}
%

\clearpage
\appendix

\section{Доказательство Теоремы~\ref{theorem}}

\begin{proof}
Рассмотрим определение M-достаточного размера выборки в терминах логарифма функции правдоподобия. В модели линейной регрессии
    \[ L\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = p(\mathbf{y} | \mathbf{X}, \hat{\mathbf{w}}_k) = \prod_{i=1}^{m} p(y_i | \mathbf{x}_i, \hat{\mathbf{w}}_k) = \prod_{i=1}^{m} \mathcal{N}\left( y_i | \hat{\mathbf{w}}_k^{\top} \mathbf{x}_i, \sigma^2 \right) = \]
    \[= \left(2\pi\sigma^2 \right)^{-m/2} \exp\left(-\dfrac{1}{2\sigma^2}\|\mathbf{y} -\mathbf{X} \hat{\mathbf{w}}_k\|_2^2 \right). \]
Прологарифмируем:
    \[ l\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = \log p(\mathbf{y} | \mathbf{X}, \hat{\mathbf{w}}_k) = -\dfrac{m}{2}\log\left( 2\pi\sigma^2 \right) - \dfrac{1}{2\sigma^2} \| \mathbf{y} - \mathbf{X} \hat{\mathbf{w}}_k \|_2^2. \]
Возьмем математическое ожидание по $\mathfrak{D}_k$, учитывая, что $\mathbb{E}_{\mathfrak{D}_k}\hat{\mathbf{w}}_k=\mathbf{m}_k$ и $\text{cov}(\hat{\mathbf{w}}_k) = \mathbf{\Sigma}_k$:
    \[ \mathbb{E}_{\mathfrak{D}_k} l\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = -\dfrac{m}{2}\log\left( 2\pi\sigma^2 \right) - \dfrac{1}{2\sigma^2} \Big( \| \mathbf{y} - \mathbf{X} \mathbf{m}_k \|_2^2 + \text{tr}\left( \mathbf{X}^{\top}\mathbf{X} \mathbf{\Sigma}_k \right) \Big). \]
    Запишем выражение для разности математических ожиданий:
    \[ \mathbb{E}_{\mathfrak{D}_{k+1}} l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\mathfrak{D}_k} l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) = \]
    \[ = \dfrac{1}{2\sigma^2} \Big( \| \mathbf{y} - \mathbf{X} \mathbf{m}_k \|_2^2 - \| \mathbf{y} - \mathbf{X} \mathbf{m}_{k+1} \|_2^2 \Big) + \dfrac{1}{2\sigma^2} \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \Big) \right) = \]
    \[ = \dfrac{1}{2\sigma^2} \Big( 2 \mathbf{y}^{\top} \mathbf{X} (\mathbf{m}_{k+1} - \mathbf{m}_k) + (\mathbf{m}_k - \mathbf{m}_{k+1})^{\top} \mathbf{X}^{\top}\mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \Big) + \]
    \[ + \dfrac{1}{2\sigma^2} \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big). \]
Значение функции $M(k)$ есть модуль от вышеприведенного выражения. Применим неравенство треугольника для модуля, а затем оценим каждое слагаемое.\\
    Первое слагаемое оценим, используя неравенство Коши-Буняковского:
\[\big| \mathbf{y}^{\top}\mathbf{X}(\mathbf{m}_{k+1}-\mathbf{m}_k)\big| \leqslant \| \mathbf{X}^{\top}\mathbf{y} \|_2 \|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2. \]
    Второе слагаемое оценим, используя неравенство Коши-Буняковского, свойство согласованности спектральной матричной нормы, а также ограниченность последовательности векторов $\bm_k$, которая следует из предъявленной в условии сходимости:
\[\big| (\mathbf{m}_k - \mathbf{m}_{k+1})^{\top} \mathbf{X}^{\top}\mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \big| \leqslant \| \mathbf{X} (\mathbf{m}_k - \mathbf{m}_{k+1}) \|_2 \| \mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \|_2 \leqslant \]
    \[ \leqslant \| \mathbf{X} \|_2^2 \| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2 \| \mathbf{m}_k + \mathbf{m}_{k+1} \|_2 \leqslant C \| \mathbf{X} \|_2^2 \| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2. \]
    Последнее слагаемое оценим, используя неравенство Гельдера для нормы Фробениуса:
    \[ \Big| \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big) \Big| \leqslant \| \mathbf{X}^{\top}\mathbf{X} \|_F \| \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \|_F. \]
Наконец, поскольку $\| \bm_k - \bm_{k+1} \|_2 \to 0$ и $\| \bSigma_k - \bSigma_{k+1} \|_{F} \to 0$ при $k \to \infty$, то $M(k) \to 0$ при $k \to \infty$, что доказывает теорему.
\end{proof}

\section{Доказательство Следствия~\ref{corollary}}

\begin{proof}
    Из приведенных в условии сходимостей следует, что $\| \bm_k - \bm_{k+1} \|_2 \to 0$ и $\| \bSigma_k - \bSigma_{k+1} \|_{F} \to 0$ при $k \to \infty$. Применение Теоремы~\ref{theorem} заканчивает доказательство.
\end{proof}

\end{document}
