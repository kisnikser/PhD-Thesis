@article{kiselev2024unraveling,
  title   = {Unraveling the Hessian: A Key to Smooth Convergence in Loss Function Landscapes},
  author  = {Kiselev, Nikita and Grabovoy, Andrey},
  year    = {2024},
  journal = {arXiv preprint arXiv:2409.11995},
  note    = {Upper bounds via Hessian for fully connected neural networks.}
}

@misc{meshkov2024convnets,
  title  = {ConvNets Landscape Convergence: Hessian-Based Analysis of Matricized Networks},
  author = {Meshkov, Vladislav and Kiselev, Nikita and Grabovoy, Andrey},
  year   = {2024},
  note   = {Upper bounds via Hessian for convolutional neural networks.},
  url    = {https://ieeexplore.ieee.org/document/10899113}
}

@article{ormaniec2024attentionhessian,
  title   = {What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis},
  author  = {Weronika Ormaniec and Felix Dangel and Sidak Pal Singh},
  year    = {2024},
  journal = {arXiv preprint arXiv:2410.10986},
  note    = {Self-Attention Block decomposition}
}

@inproceedings{
schaipp2025the,
title={The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training},
author={Fabian Schaipp and Alexander H{\"a}gele and Adrien Taylor and Umut Simsekli and Francis Bach},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=b836TGkRSw}
}

@inproceedings{lecun1989obd,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 volume = {2},
 year = {1989}
}

@misc{berahas2020lbfgs,
      title={Limited-Memory BFGS with Displacement Aggregation}, 
      author={Albert S. Berahas and Frank E. Curtis and Baoyu Zhou},
      year={2020},
      eprint={1903.03471},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1903.03471}, 
}

@inproceedings{
vyas2025soap,
title={{SOAP}: Improving and Stabilizing Shampoo using Adam for Language Modeling},
author={Nikhil Vyas and Depen Morwani and Rosie Zhao and Itai Shapira and David Brandfonbrener and Lucas Janson and Sham M. Kakade},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=IDxZhXrpNf}
}

@misc{martens2020kfac,
      title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature}, 
      author={James Martens and Roger Grosse},
      year={2020},
      eprint={1503.05671},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1503.05671}, 
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@book{magnus1988matrix,
  title={Matrix Differential Calculus with Applications in Statistics and Econometrics},
  author={Magnus, Jan R. and Neudecker, Heinz},
  year={1988},
  publisher={Wiley},
  address={Chichester},
  isbn={9780471915163}
}

@book{nocedal2006numericaloptimization,
  title={Numerical Optimization},
  author={Jorge Nocedal, Stephen J. Wright},
  year={2006},
  publisher={Springer},
  DOI={10.1007/978-0-387-40065-5}
}

@book{horn2012matrixanalysis,
  title={Matrix Analysis},
  author={Roger A. Horn, Charles R. Johnson},
  year={2012},
  publisher={Cambridge University Press},
  isbn={9781139788885}
}

@book{nesterov2013convexoptimization,
  title={Introductory Lectures on Convex Optimization: A Basic Course},
  author={Y. Nesterov},
  year={2013},
  publisher={Springer Science & Business Media},
  isbn={9781441988539}
}

@book{boyd2004convex,
  title={Convex Optimization},
  author={Stephen Boyd, Lieven Vandenberghe},
  year={2004},
  publisher={Cambridge University Press},
  isbn={9781107394001}
}

@book{goodfellow2016deeplearning,
  title={Deep Learning. Adaptive Computation and Machine Learning series},
  author={Ian Goodfellow, Yoshua Bengio, Aaron Courville},
  year={2016},
  publisher={MIT Press},
  isbn={9780262035613}
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@article{burke2025newtonmethod,
author = {Burke, James and Ferris, Michael},
year = {1995},
month = {12},
pages = {179-194},
title = {A Gauss—Newton method for convex composite optimization},
volume = {71},
journal = {Mathematical Programming},
doi = {10.1007/BF01585997}
}

@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de чLas Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@misc{dauphin2014computationalcomplexity,
      title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}, 
      author={Yann Dauphin and Razvan Pascanu and Caglar Gulcehre and Kyunghyun Cho and Surya Ganguli and Yoshua Bengio},
      year={2014},
      eprint={1406.2572},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1406.2572}, 
}

@article{schraudolph2002aproxhessian,
author = {Schraudolph, Nicol},
year = {2002},
month = {07},
pages = {1723-1738},
title = {Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent},
volume = {14},
journal = {Neural Computation},
doi = {10.1162/08997660260028683}
}

@article{pearlmutter1994multiplicationhessian,
author = {Pearlmutter, Barak},
year = {1994},
month = {01},
pages = {147-160},
title = {Fast Exact Multiplication by the Hessian},
volume = {6},
journal = {Neural Computation},
doi = {10.1162/neco.1994.6.1.147}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{dosovitskiy2021image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}

@misc{rogers2020bertology,
      title={A Primer in BERTology: What we know about how BERT works}, 
      author={Anna Rogers and Olga Kovaleva and Anna Rumshisky},
      year={2020},
      eprint={2002.12327},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.12327}, 
}

@InProceedings{xiong2020prelayernorm,
  title = 	 {On Layer Normalization in the Transformer Architecture},
  author =       {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10524--10533},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf},
  url = 	 {https://proceedings.mlr.press/v119/xiong20b.html},
}

@misc{yarmoshik2024decentralizedoptimizationcoupledconstraints,
      title={Decentralized Optimization with Coupled Constraints}, 
      author={Demyan Yarmoshik and Alexander Rogozin and Nikita Kiselev and Daniil Dorin and Alexander Gasnikov and Dmitry Kovalev},
      year={2024},
      eprint={2407.02020},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2407.02020}, 
}


@article{li2023theoreticalunderstandingshallowvision,
  title   = {A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity},
  author  = {Li, Hongkang and Xu, Meng and Wang, Tianyang and Yang, Shuai and Shen, Feng and Xu, Wei and Darrell, Trevor},
  year    = {2023},
  journal = {OpenReview},
  url     = {https://openreview.net/forum?id=jClGv3Qjhb}
}

@article{anonymous2024stagewisedevelopmenttransformers,
title={Stagewise Development in Transformers and the Geometry of the Loss Landscape},
author={Jesse Hoogland and George Wang and Matthew Farrugia-Roberts and Liam Carroll and Susan Wei and Daniel Murfet},
year={2025},
url={https://openreview.net/forum?id=xEZiEhjTeq}
}

@article{draxler2019essentiallynobarriers,
  title   = {Essentially No Barriers in Neural Network Energy Landscape},
  author  = {Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred A.},
  year    = {2019},
  journal = {arXiv preprint arXiv:1803.00885},
  url     = {https://arxiv.org/abs/1803.00885}
}

@article{garipov2018losssurfacesmodeconnectivity,
  title   = {Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs},
  author  = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P. and Wilson, Andrew Gordon},
  year    = {2018},
  journal = {arXiv preprint arXiv:1802.10026},
  url     = {https://arxiv.org/abs/1802.10026}
}

@article{nguyen2017losssurfacedeepwide,
  title   = {The Loss Surface of Deep and Wide Neural Networks},
  author  = {Nguyen, Quynh and Mukkamala, Mahesh Chandra and Hein, Matthias},
  year    = {2017},
  journal = {arXiv preprint arXiv:1704.08045},
  url     = {https://arxiv.org/abs/1704.08045}
}

@article{xie2024losslens,
  title   = {LossLens: Diagnostics for Machine Learning through Loss Landscape Visual Analytics},
  author  = {Xie, Tiankai and Li, Xiangyu and Zhang, Yan and Wang, Yiming and Zhang, Hao and Liu, Mingyuan and Zhang, Jie},
  year    = {2024},
  journal = {arXiv preprint arXiv:2412.13321},
  url     = {https://arxiv.org/abs/2412.13321}
}



@misc{wu2020visualtransformer,
      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, 
      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},
      year={2020},
      eprint={2006.03677},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2006.03677}, 
}


@InProceedings{moreno2024medicalfewshot,
  title = 	 {Few-Shot Learning with Semi-Supervised Transformers for Electronic Health Records},
  author =       {Poulain, Raphael and Gupta, Mehak and Beheshti, Rahmatollah},
  booktitle = 	 {Proceedings of the 7th Machine Learning for Healthcare Conference},
  pages = 	 {853--873},
  year = 	 {2022},
  editor = 	 {Lipton, Zachary and Ranganath, Rajesh and Sendak, Mark and Sjoding, Michael and Yeung, Serena},
  volume = 	 {182},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {05--06 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v182/poulain22a/poulain22a.pdf},
  url = 	 {https://proceedings.mlr.press/v182/poulain22a.html},
  abstract = 	 {With the growing availability of Electronic Health Records (EHRs), many deep learning methods have been developed to leverage such datasets in medical prediction tasks. Notably, transformer-based architectures have proven to be highly effective for EHRs. Transformer-based architectures are generally very effective in “transferring” the acquired knowledge from very large datasets to smaller target datasets through their comprehensive “pre-training” process. However, to work efficiently, they still rely on the target datasets for the downstream tasks, and if the target dataset is (very) small, the performance of downstream models can degrade rapidly. In biomedical applications, it is common to only have access to small datasets, for instance, when studying rare diseases, invasive procedures, or using restrictive cohort selection processes. In this study, we present CEHR-GAN-BERT, a semi-supervised transformer-based architecture that leverages both in and out-of-cohort patients to learn better patient representations in the context of few-shot learning. The proposed method opens new learning opportunities where only a few hundred samples are available. We extensively evaluate our method on four prediction tasks and three public datasets showing the ability of our model to achieve improvements upwards of 5% on all performance metrics (including AUROC and F1 Score) on the tasks that use less than 200 annotated patients during the training process.}
}


@misc{noci2022signalpropagationtransformerstheoretical,
      title={Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse}, 
      author={Lorenzo Noci and Sotiris Anagnostidis and Luca Biggio and Antonio Orvieto and Sidak Pal Singh and Aurelien Lucchi},
      year={2022},
      eprint={2206.03126},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.03126}, 
}

@misc{singh2021analyticinsightsstructurerank,
      title={Analytic Insights into Structure and Rank of Neural Network Hessian Maps}, 
      author={Sidak Pal Singh and Gregor Bachmann and Thomas Hofmann},
      year={2021},
      eprint={2106.16225},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.16225}, 
}

@misc{petersen2012matrix,
  title        = {The Matrix Cookbook},
  author       = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  year         = {2012},
  howpublished = {\url{https://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf}},
  note         = {Version November 15, 2012}
}


% NEW

@misc{jumper2021highly,
  title = {Highly Accurate Protein Structure Prediction with AlphaFold},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and others},
  year = {2021},
  note = {Transformer-based protein structure prediction.},
  url = {https://www.nature.com/articles/s41586-021-03819-2}
}

@misc{hochreiter1998vanishing,
  title = {The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions},
  author = {Hochreiter, Sepp},
  year = {1998},
  note = {Analysis of vanishing gradient issues.},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0888613X98000140}
}
@misc{mccandlish2018empirical,
  title = {An Empirical Model of Large-Batch Training},
  author = {McCandlish, Sam and others},
  year = {2018},
  note = {Empirical analysis of large-batch training dynamics.},
  url = {https://arxiv.org/abs/1812.06162}
}
@misc{sagun2017eigenvalues,
  title = {Eigenvalues of the Hessian in Deep Learning},
  author = {Sagun, Levent and others},
  year = {2017},
  note = {Hessian eigenvalue analysis for large-scale networks.},
  url = {https://arxiv.org/abs/1706.04454}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{bahri2024explainingscalinglaws,
   title={Explaining neural scaling laws},
   volume={121},
   ISSN={1091-6490},
   url={http://dx.doi.org/10.1073/pnas.2311878121},
   DOI={10.1073/pnas.2311878121},
   number={27},
   journal={Proceedings of the National Academy of Sciences},
   publisher={Proceedings of the National Academy of Sciences},
   author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
   year={2024},
   month=jun }


@inproceedings{
zhang2025understanding,
title={Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study},
author={Xingxuan Zhang and Haoran Wang and Jiansheng Li and Yuan Xue and Shikai Guan and Renzhe Xu and Hao Zou and Han Yu and Peng Cui},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=yOhNLIqTEF}
}

@misc{chen2025dataefficientgeneralizationzeroshotcomposed,
      title={Data-Efficient Generalization for Zero-shot Composed Image Retrieval}, 
      author={Zining Chen and Zhicheng Zhao and Fei Su and Xiaoqin Zhang and Shijian Lu},
      year={2025},
      eprint={2503.05204},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.05204}, 
}

@inproceedings{
zhang2024why,
title={Why Transformers Need Adam: A Hessian Perspective},
author={Yushun Zhang and Congliang Chen and Tian Ding and Ziniu Li and Ruoyu Sun and Zhi-Quan Luo},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=X6rqEpbnj3}
}

@unknown{unknown,
author = {Yang, Hongru and Kailkhura, Bhavya and Wang, Zhangyang and Liang, Yingbin},
year = {2024},
month = {10},
pages = {},
title = {Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis},
doi = {10.48550/arXiv.2410.09605}
}

@misc{zhang2025understandinggeneralizationincontextlearning,
      title={Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study}, 
      author={Xingxuan Zhang and Haoran Wang and Jiansheng Li and Yuan Xue and Shikai Guan and Renzhe Xu and Hao Zou and Han Yu and Peng Cui},
      year={2025},
      eprint={2503.15579},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.15579}, 
}

@inproceedings{csordas-etal-2021-devil,
    title = "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers",
    author = "Csord{\'a}s, R{\'o}bert  and
      Irie, Kazuki  and
      Schmidhuber, Juergen",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.49/",
    doi = "10.18653/v1/2021.emnlp-main.49",
    pages = "619--634",
    abstract = "Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50{\%} to 85{\%} on the PCFG productivity split, and from 35{\%} to 81{\%} on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100{\%} accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results."
}

@misc{fort2019emergentpropertieslocalgeometry,
      title={Emergent properties of the local geometry of neural loss landscapes}, 
      author={Stanislav Fort and Surya Ganguli},
      year={2019},
      eprint={1910.05929},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.05929}, 
}


@misc{li2023transformersalgorithmsgeneralizationstability,
      title={Transformers as Algorithms: Generalization and Stability in In-context Learning}, 
      author={Yingcong Li and M. Emrullah Ildiz and Dimitris Papailiopoulos and Samet Oymak},
      year={2023},
      eprint={2301.07067},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.07067}, 
}

@inproceedings{
gao2024global,
title={Global Convergence in Training Large-Scale Transformers},
author={Cheng Gao and Yuan Cao and Zihao Li and Yihan He and Mengdi Wang and Han Liu and Jason Matthew Klusowski and Jianqing Fan},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=9wtlfRKwZS}
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@inproceedings{
zhang2025how,
title={How Does Critical Batch Size Scale in Pre-training?},
author={Hanlin Zhang and Depen Morwani and Nikhil Vyas and Jingfeng Wu and Difan Zou and Udaya Ghai and Dean Foster and Sham M. Kakade},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=JCiF03qnmi}
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@inproceedings{krizhevsky2009learning,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:18268744}
}