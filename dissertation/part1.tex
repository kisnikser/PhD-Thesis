\chapter{Обзор проблемы и постановка задач исследования}\label{ch:intro}

В данной главе приводится введение в задачу определения достаточного размера обучающей выборки для параметрических моделей машинного обучения и постановка задач диссертационной работы. Сначала вводятся основные понятия: параметрическая модель, функция потерь, множество оптимальных параметров и его зависимость от выборки. Далее дается краткий обзор существующих подходов к оценке достаточного размера выборки и их ограничений для современных моделей. Затем вводится ключевой объект исследования~--- оптимизационная поверхность (ландшафт функции потерь)~--- и обсуждается, как ее стабилизация при увеличении объема данных может служить признаком достаточности выборки. В заключение главы формулируются критерии достаточности для линейных моделей и нейронных сетей, а также связь с законами масштабирования. Детальное обсуждение и доказательства приводятся в последующих главах.

\section{Параметрические модели машинного обучения и задача оптимизации}

Решение задачи машинного обучения с учителем предполагает выбор предсказательной модели из некоторого параметрического семейства~\cite{vapnik1995nature}. Объектом называется пара $(\mathbf{x}, y)$, где $\mathbf{x} \in \mathbb{X} \subseteq \mathbb{R}^n$~--- вектор признаков, $y \in \mathbb{Y}$~--- целевая переменная. В задаче регрессии $\mathbb{Y} = \mathbb{R}$, в задаче классификации $\mathbb{Y} = \{1, \ldots, K\}$. Выборка размера $m$ задается множеством $\mathfrak{D}_m = \{(\mathbf{x}_i, y_i)\}_{i=1}^m$ из $m$ независимых реализаций пар $(\mathbf{x}, y)$, распределенных согласно неизвестному распределению $p(\mathbf{x}, y)$.

Параметрической моделью называется семейство функций $f_{\mathbf{w}}: \mathbb{X} \to \mathbb{Y}$, задаваемое параметром $\mathbf{w} \in \mathbb{R}^P$. Обучение модели состоит в выборе параметров, минимизирующих эмпирическую функцию потерь на выборке:
\begin{equation}
\label{eq:empirical-loss}
\mathcal{L}_m(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^m \ell(f_{\mathbf{w}}(\mathbf{x}_i), y_i),
\end{equation}
где $\ell: \mathbb{Y} \times \mathbb{Y} \to \mathbb{R}^+$~--- функция потерь на одном объекте (например, квадратичная ошибка или кросс-энтропия). При увеличении размера выборки эмпирическая функция потерь \eqref{eq:empirical-loss} сходится к популяционной $\mathcal{L}(\mathbf{w}) = \mathbb{E}_{(\mathbf{x},y) \sim p}[\ell(f_{\mathbf{w}}(\mathbf{x}), y)]$. Таким образом, обучение~--- это оптимизационная задача: найти $\mathbf{w}^* \in \argmin_{\mathbf{w}} \mathcal{L}_m(\mathbf{w})$.

Важно отметить, что эмпирическая функция потерь вычисляется по конечной выборке и потому случайна: при другой реализации выборки $\mathfrak{D}_m'$ того же размера получается другая функция $\mathcal{L}_m'(\mathbf{w})$ и, как правило, другое множество минимизаторов. Это обстоятельство лежит в основе проблемы определения достаточного размера выборки.

Множество минимизаторов $\argmin_{\mathbf{w}} \mathcal{L}_m(\mathbf{w})$ определяет те параметры, которые модель <<выбирает>> в результате обучения. При малом $m$ это множество может быть обширным или нестабильным: добавление или удаление нескольких объектов существенно меняет и значение минимума, и расположение оптимальных параметров. При увеличении $m$ эмпирическая функция потерь сходится к популяционной, и естественно ожидать, что множество решений стабилизируется~--- сужается или перестает заметно смещаться при добавлении новых данных.

В вероятностной постановке, когда модель задается правдоподобием $p(y|\mathbf{x},\mathbf{w})$ и априорным распределением $p(\mathbf{w})$, оптимальные параметры описываются апостериорным распределением $p(\mathbf{w}|\mathfrak{D}_m)$. При увеличении объема выборки апостериорное распределение, как правило, концентрируется около <<истинных>> параметров, а его дисперсия уменьшается. Стабилизация апостериорного распределения при добавлении данных~--- альтернативная характеристика достаточности выборки, используемая в байесовском подходе~\cite{kaplan2020scaling}.

В задачах машинного обучения оптимум функции потерь часто не единствен. В линейной регрессии при невырожденной матрице признаков существует единственный глобальный минимум, но при мультиколлинеарности или при $m < n$ задача вырождена: множество решений образует аффинное подпространство. В нейронных сетях функция потерь, как правило, невыпукла: существуют множественные локальные минимумы, седловые точки и плато. Более того, из-за симметрий (перестановка нейронов, знаковые преобразования) многие минимумы эквивалентны с точки зрения предсказательной способности~\cite{draxler2018essentially}. В таких условиях корректнее говорить не о единственной точке минимума, а о множестве эквивалентных решений или о распределении решений в пространстве параметров.

Вырожденность и неединственность затрудняют применение классических статистических критериев, ориентированных на единственную оценку параметров. Поэтому для характеристики достаточности выборки целесообразно использовать свойства, инвариантные к выбору конкретного минимума: например, геометрию поверхности функции потерь в окрестности решений или распределение решений по подвыборкам.

\section{Исторический обзор подходов к определению достаточного размера выборки}

Классические статистические методы оценки размера выборки опираются на задание нулевой и альтернативной гипотез, допустимой вероятности ошибки первого рода и требуемой мощности теста~\cite{vapnik1995nature}~\todo{(уточнить ссылку)}. К ним относятся метод множителей Лагранжа, отношение правдоподобия, критерий Вальда и др. Эти методы дают теоретически обоснованные оценки, но требуют явных предположений о распределении данных и параметрах модели, а также информации о величине эффекта при альтернативе. На практике такие предположения часто недоступны или труднопроверяемы.

В статистической теории обучения размер выборки связывается со сложностью класса моделей через размерность Вапника"--~Червоненкиса (VC) и родственные понятия. Получены оценки достаточного объема данных для гарантированного обобщения в рамках PAC-анализа. Однако для глубоких нейронных сетей VC-размерность огромна и дает слишком пессимистичные оценки; кроме того, современные переобученные сети часто хорошо обобщают, что не согласуется с классическими границами~\cite{kaplan2020scaling}~\todo{(уточнить ссылку)}.

Информационно-теоретические подходы характеризуют связь между данными и моделью через энтропию, взаимную информацию или дивергенции между распределениями~\todo{(добавить ссылки)}. Байесовские методы используют апостериорное распределение параметров и критерии вроде средней апостериорной дисперсии (APVC), среднего покрытия (ACC) или дивергенции Кульбака"--~Лейблера между апостериорными распределениями на разных подвыборках~\todo{(добавить ссылки)}. В работе~\cite{MOTRENKO2014743} для логистической регрессии предложено использовать KL-дивергенцию между апостериорами на <<схожих>> подвыборках (отличающихся одним объектом) для оценки достаточного размера выборки. Эти идеи развиты в диссертационной работе для вероятностной линейной регрессии с теоретическим обоснованием.

Классические подходы плохо переносятся на современные нейронные сети по нескольким причинам. Во-первых, VC-размерность и сходные меры сложности для DNN дают нереалистично большие оценки необходимого объема данных. Во-вторых, явные статистические гипотезы о параметрах нейросетей обычно неизвестны. В-третьих, многократное полное переобучение модели на подвыборках разного размера для построения кривой обучения вычислительно  дорого для больших сетей.

В то же время эмпирические законы масштабирования показывают устойчивые зависимости: качество моделей закономерно улучшается с ростом размера данных и числа параметров. В языковых моделях установлены степенные законы~\cite{kaplan2020scaling}, а для вычисленно-оптимального обучения предложено согласованное масштабирование модели и объема данных~\cite{hoffmann2022chinchila}. Эти результаты пока не имеют единого теоретического обоснования, связывающего геометрию оптимизационной задачи с объемом выборки. Таким образом, возникает необходимость в новых подходах, опирающихся на объекты, доступные для анализа в нейросетях~--- например, на поверхность функции потерь.

\section{Оптимизационная поверхность как характеристика модели}

Ландшафтом (оптимизационной поверхностью) функции потерь называют график $\mathcal{L}_m(\mathbf{w})$ как функции от $\mathbf{w} \in \mathbb{R}^P$. Визуализация и анализ этой поверхности для нейронных сетей активно изучаются~\cite{li2018visualizing,fort2019large}. Ключевые характеристики~--- наличие множественных минимумов, седловых точек, <<узких>> и <<широких>> долин. Плоские минимумы (с малой кривизной) часто связывают с лучшей обобщающей способностью~\cite{chaudhari2019entropy}. Связность множества решений (возможность перейти от одного минимума к другому без существенного роста потерь) пропдемонстрирована в~\cite{draxler2018essentially,garipov2018loss}.

Кривизна поверхности в окрестности минимума описывается матрицей Гессе $\mathbf{H} = \nabla^2_{\mathbf{w}} \mathcal{L}_m(\mathbf{w})$. Спектр Гессе характеризует жесткость минимума: большие собственные значения соответствуют <<острым>> направлениям, малые~--- <<плоским>>. Для типичных нейросетей наблюдается характерная структура спектра: множество малых собственных значений и небольшое число выбросов~\cite{papyan2019spectrum}. Эта структура используется при анализе обобщения и сходимости оптимизации.

Структура поверхности функции потерь отражает архитектуру модели: глубину, ширину, тип слоев. Для полносвязных сетей получены оценки спектральной нормы матрицы Гессе через число слоев и размер скрытых слоев; для сверточных сетей~--- через параметры сверток и пулинга. Таким образом, геометрия ландшафта функции потерь может служить характеристикой <<сложности>> модели в смысле чувствительности к возмущениям параметров и данным.

При увеличении $m$ эмпирическая функция потерь $\mathcal{L}_m(\mathbf{w})$ сходится к популяционной $\mathcal{L}(\mathbf{w})$, и естественно ожидать, что сама поверхность <<стабилизируется>>: разность $\mathcal{L}_{m+1}(\mathbf{w}) - \mathcal{L}_m(\mathbf{w})$ уменьшается при росте $m$, особенно в окрестности минимума. Такая стабилизация может служить признаком достаточности выборки: если добавление нового объекта почти не меняет поверхность функции потерь в области решений, то выборка, по-видимому, уже достаточно представительна.

Теоретический анализ этой идеи требует оценки скорости сходимости разности потерь и связи с матрицей Гессе. В настоящей работе для полносвязных сетей получены верхние оценки разности $\mathcal{L}_{m+1}(\mathbf{w}) - \mathcal{L}_m(\mathbf{w})$ через спектральную норму Гессе, что дает зависимость от архитектуры сети и обратную зависимость от $m$. Эти результаты обобщены на сверточные архитектуры и позволяют теоретически обосновать использование сходимости поверхности функции потерь как признака достаточного размера выборки.

\section{Постановка задач исследования}

В диссертации рассматриваются следующие критерии достаточности выборки.

Для вероятностной модели линейной регрессии с правдоподобием $L(\mathfrak{D}_m, \mathbf{w})$ и оценкой максимального правдоподобия $\hat{\mathbf{w}}_k$ по подвыборке размера $k$:

\begin{enumerate}
\item \textbf{D-достаточность}: дисперсия $D(k) = \mathbb{D}_{\hat{\mathbf{w}}_k} L(\mathfrak{D}_m, \hat{\mathbf{w}}_k)$ не превосходит заданного порога~$\varepsilon$ для всех $k \geqslant m^*$.
\item \textbf{M-достаточность}: разность математических ожиданий $M(k) = |\mathbb{E} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E} L(\mathfrak{D}_m, \hat{\mathbf{w}}_k)|$ не превосходит~$\varepsilon$ для всех $k \geqslant m^*$.
\item \textbf{KL-достаточность}: KL-дивергенция между апостериорными распределениями параметров на подвыборках размера~$k$ и $k+1$ не превосходит~$\varepsilon$ для всех $k \geqslant m^*$.
\item \textbf{S-достаточность}: функция близости s-score между этими апостериорами не менее ~$1 - \varepsilon$ для всех $k \geqslant m^*$.
\end{enumerate}

Критерии D и M опираются на бутстрапирование правдоподобия; критерии KL и S~--- на близость апостериорных распределений на <<схожих>> подвыборках. В данной диссертации для всех четырех критериев \textbf{получены теоретические оценки корректности и верхние границы} зависимости~$m^*$ от~$\varepsilon$ (глава~\ref{ch:linear} работы).

Для матрично-представимых нейронных сетей (полносвязных, сверточных, трансформеров) сходимость поверхности функции потерь при увеличении $m$ формализуется через оценку
\[
|\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})| \leqslant \frac{C}{k+1}
\]
в окрестности минимума $\mathbf{w}^*$, где константа $C$ зависит от спектральной нормы матрицы Гессе и радиуса окрестности. Используется разложение Гаусса"--~Ньютона и спектральный анализ Гессе. В настоящей диссертации \textbf{получены оценки для различных архитектур} (глава~\ref{ch:landscapes} работы). Численная проверка сходимости выполняется сравнением значений функции потерь в точке минимума при добавлении новых данных.

Для более робастной оценки сходимости в диссертации \textbf{предлагается  распределенный подход}: сэмплирование точек по Монте"--~Карло в гауссовской окрестности минимума и усреднение разности потерь (глава~\ref{ch:robust} работы). Переход к подпространству главных собственных векторов Гессе позволяет снизить вычислительную сложность при сохранении точности оценок.

Стабилизация поверхности функции потерь при увеличении объема выборки тесно связана со стабилизацией множества оптимальных параметров. Если поверхность в окрестности минимума перестает существенно меняться при добавлении данных, то и множество решений (в смысле эквивалентных минимумов или распределения апостериора) стабилизируется. Таким образом, сходимость оптимизационной поверхности можно рассматривать как геометрический признак достаточности выборки, единый для линейных и нейросетевых моделей.

Эта связь дает теоретическое обоснование эмпирических законов масштабирования~\cite{hoffmann2022chinchila}: согласованное масштабирование модели и объема данных обеспечивает, что поверхность функции потерь успевает стабилизироваться при росте сложности модели. Разработанные в диссертации методы позволяют количественно оценить, при каком объеме выборки такая стабилизация достигается для заданной модели и порога $\varepsilon$.
