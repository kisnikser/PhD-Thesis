\chapter{Распределение оптимальных параметров в линейных моделях}\label{ch:linear}

В данной главе рассматриваются методы определения достаточного размера обучающей выборки для вероятностных линейных моделей. Сначала вводится вероятностная постановка линейной регрессии и описывается зависимость апостериорного распределения параметров от размера выборки. Далее формулируются четыре критерия достаточности: D-достаточность (дисперсия правдоподобия), M-достаточность (изменение математического ожидания правдоподобия), KL-достаточность (расхождение Кульбака"--~Лейблера между апостериорами) и S-достаточность (метрика близости s-score). Для каждого критерия приводятся теоремы о корректности и условия сходимости в модели линейной регрессии. В заключение обсуждается обобщение на логистическую регрессию и обобщенные линейные модели.

\section{Введение}

TODO

\section{Обзор существующих методов}

TODO

\section{Предварительные сведения}

\textbf{Вероятностная модель линейной регрессии.} В задаче регрессии объект~--- это пара $(\mathbf{x}, y)$, где $\mathbf{x} \in \mathbb{R}^n$~--- вектор признаков, $y \in \mathbb{R}$~--- целевая переменная. Выборка размера $m$ задается множеством $\mathfrak{D}_m = \{(\mathbf{x}_i, y_i)\}_{i=1}^m$. Матрица объекты"--~признаки $\mathbf{X}_m = [\mathbf{x}_1, \ldots, \mathbf{x}_m]^\top \in \mathbb{R}^{m \times n}$ и вектор ответов $\mathbf{y}_m = [y_1, \ldots, y_m]^\top \in \mathbb{R}^m$ определяют выборку в матричной форме.

Вероятностная модель линейной регрессии задается правдоподобием
\begin{equation}
\label{eq:linreg-likelihood}
p(y | \mathbf{x}, \mathbf{w}) = \mathcal{N}(y | \mathbf{w}^\top \mathbf{x}, \sigma^2),
\end{equation}
где $\mathbf{w} \in \mathbb{R}^n$~--- вектор параметров, $\sigma^2 > 0$~--- известная дисперсия шума. Для простой выборки функция правдоподобия имеет вид
\begin{equation}
\label{eq:likelihood-full}
L(\mathfrak{D}_m, \mathbf{w}) = p(\mathbf{y}_m | \mathbf{X}_m, \mathbf{w}) = (2\pi\sigma^2)^{-m/2} \exp\left(-\frac{1}{2\sigma^2} \|\mathbf{y}_m - \mathbf{X}_m \mathbf{w}\|_2^2\right),
\end{equation}
а логарифмическая функция правдоподобия
\begin{equation}
\label{eq:loglikelihood}
l(\mathfrak{D}_m, \mathbf{w}) = \log L(\mathfrak{D}_m, \mathbf{w}) = -\frac{m}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \|\mathbf{y}_m - \mathbf{X}_m \mathbf{w}\|_2^2.
\end{equation}

Оценкой максимума правдоподобия (ММП) по подвыборке $\mathfrak{D}_k \subseteq \mathfrak{D}_m$ размера $k$ называется
\begin{equation}
\label{eq:mle}
\hat{\mathbf{w}}_k = \argmax_{\mathbf{w}} L(\mathfrak{D}_k, \mathbf{w}) = \argmin_{\mathbf{w}} \|\mathbf{y}_k - \mathbf{X}_k \mathbf{w}\|_2^2.
\end{equation}
При условии полного столбцового ранга матрицы $\mathbf{X}_k$ оценка ММП единственна и задается формулой
\begin{equation}
\hat{\mathbf{w}}_k = (\mathbf{X}_k^\top \mathbf{X}_k)^{-1} \mathbf{X}_k^\top \mathbf{y}_k.
\end{equation}

\textbf{Апостериорное распределение параметров и его зависимость от размера выборки.} При заданном априорном распределении $p(\mathbf{w})$ апостериорное распределение параметров по правилу Байеса имеет вид
\begin{equation}
\label{eq:posterior}
p(\mathbf{w} | \mathfrak{D}_k) = \frac{p(\mathfrak{D}_k | \mathbf{w}) p(\mathbf{w})}{p(\mathfrak{D}_k)} \propto L(\mathfrak{D}_k, \mathbf{w}) p(\mathbf{w}).
\end{equation}

Для гауссовского априорного распределения $p(\mathbf{w}) = \mathcal{N}(\mathbf{w} | \boldsymbol{\mu}_0, \boldsymbol{\Sigma}_0)$ и правдоподобия \eqref{eq:linreg-likelihood} апостериорное распределение сопряжено и также является гауссовским~\cite{bishop2006pattern}:
\begin{equation}
\label{eq:posterior-gaussian}
p(\mathbf{w} | \mathfrak{D}_k) = \mathcal{N}(\mathbf{w} | \mathbf{m}_k, \boldsymbol{\Sigma}_k),
\end{equation}
где
\begin{equation}
\boldsymbol{\Sigma}_k^{-1} = \boldsymbol{\Sigma}_0^{-1} + \frac{1}{\sigma^2} \mathbf{X}_k^\top \mathbf{X}_k, \qquad \mathbf{m}_k = \boldsymbol{\Sigma}_k \left( \boldsymbol{\Sigma}_0^{-1} \boldsymbol{\mu}_0 + \frac{1}{\sigma^2} \mathbf{X}_k^\top \mathbf{y}_k \right).
\end{equation}

Математическое ожидание $\mathbf{m}_k$ и матрица ковариации $\boldsymbol{\Sigma}_k$ апостериорного распределения зависят от подвыборки $\mathfrak{D}_k$. При увеличении размера выборки~$k$ матрица $\mathbf{X}_k^\top \mathbf{X}_k$ накапливает информацию о признаках, дисперсия апостериора~$\boldsymbol{\Sigma}_k$ уменьшается, а апостериорное среднее~$\mathbf{m}_k$ приближается к истинным параметрам при выполнении условий регулярности.

\textbf{Сходимость распределения оптимальных параметров.} Для частотного подхода важна сходимость оценки ММП $\hat{\mathbf{w}}_k$. Из асимптотической теории известно, что при стандартных условиях регулярности (ограниченность информационной матрицы Фишера, непрерывность и т.\,д.) выполняется
\begin{equation}
\sqrt{k} (\hat{\mathbf{w}}_k - \mathbf{w}^*) \xrightarrow{d} \mathcal{N}(0, \mathcal{I}^{-1}(\mathbf{w}^*)),
\end{equation}
где $\mathbf{w}^*$~--- истинный вектор параметров, $\mathcal{I}(\mathbf{w})$~--- информационная матрица Фишера. Таким образом, оценка ММП состоятельна и асимптотически нормальна.

Для байесовского подхода ключевым является сходимость моментов апостериорного распределения. При увеличении $k$ естественно ожидать, что $\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и $\|\boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k\|_F \to 0$, т.\,е. добавление одного объекта все меньше влияет на апостериор. Условия, при которых эта сходимость имеет место, формулируются в разделе~\ref{sec:linear-theory}.

\section{Критерии достаточности для линейных моделей}

Зафиксируем полную выборку $\mathfrak{D}_m$ и рассмотрим подвыборки $\mathfrak{D}_k \subseteq \mathfrak{D}_m$ размера $k \leqslant m$. Пусть $\hat{\mathbf{w}}_k$~--- оценка ММП по $\mathfrak{D}_k$, $p_k(\mathbf{w}) = p(\mathbf{w} | \mathfrak{D}_k)$~--- апостериорное распределение параметров.

\begin{definition}
    Размер выборки $m^*$ называется \emph{достаточным} согласно критерию $T$, если $T$ выполняется для всех $k \geqslant m^*$.
\end{definition}

Ниже вводятся четыре критерия, различающиеся тем, какую характеристику стабильности модели они используют.

\subsection{D-достаточность: дисперсия оптимальных параметров}

Идея D-критерия: при достаточном размере выборки оценка параметров мало меняется от одной реализации подвыборки к другой, поэтому дисперсия значения правдоподобия (или его логарифма) в точке оценки по подвыборкам не должна превышать порога.

\begin{definition}
    Зафиксируем $\varepsilon > 0$. Размер выборки $m^*$ называется \emph{D-достаточным}, если для всех $k \geqslant m^*$
    \begin{equation}
    \label{eq:D-sufficient}
    D(k) = \mathbb{D}_{\hat{\mathbf{w}}_k} L(\mathfrak{D}_m, \hat{\mathbf{w}}_k) \leqslant \varepsilon,
    \end{equation}
    где дисперсия берется по распределению оценки $\hat{\mathbf{w}}_k$ (индуцированному случайностью подвыборки $\mathfrak{D}_k$).
\end{definition}

Вместо $L(\mathfrak{D}_m, \hat{\mathbf{w}}_k)$ можно рассматривать логарифм $l(\mathfrak{D}_m, \hat{\mathbf{w}}_k)$; выбор той или иной формы влияет на масштаб порога $\varepsilon$.

\subsection{M-достаточность: изменение математического ожидания}

M-критерий фиксирует малое изменение математического ожидания правдоподобия при добавлении одного объекта в подвыборку.

\begin{definition}
    Зафиксируем $\varepsilon > 0$. Размер выборки $m^*$ называется \emph{M-достаточным}, если для всех $k \geqslant m^*$
    \begin{equation}
    \label{eq:M-sufficient}
    M(k) = \left| \mathbb{E} L(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E} L(\mathfrak{D}_m, \hat{\mathbf{w}}_k) \right| \leqslant \varepsilon,
    \end{equation}
    где математическое ожидание берется по подвыборкам $\mathfrak{D}_{k+1}$ и $\mathfrak{D}_k$ соответственно.
\end{definition}

Если $\mathfrak{D}_{k+1}$ получается добавлением одного объекта к $\mathfrak{D}_k$, то M-критерий формализует интуицию: при достаточном объеме данных добавление очередного объекта почти не меняет ожидаемое значение правдоподобия на полной выборке.

\subsection{KL-достаточность: расхождение Кульбака"--~Лейблера апостериорных распределений}

В работе~\cite{MOTRENKO2014743} для логистической регрессии предложено использовать KL-дивергенцию между апостериорными распределениями на <<схожих>> подвыборках (отличающихся одним объектом). Подвыборки $\mathfrak{D}^1$ и $\mathfrak{D}^2$ называются \emph{схожими}, если множества их индексов отличаются ровно одним элементом: $|\mathcal{I}_1 \triangle \mathcal{I}_2| = 1$.

\begin{definition}
    Зафиксируем $\varepsilon > 0$. Размер выборки $m^*$ называется \emph{KL-достаточным}, если для всех $k \geqslant m^*$ при схожих подвыборках $\mathfrak{D}_k$ и $\mathfrak{D}_{k+1}$ выполняется
    \begin{equation}
    \label{eq:KL-sufficient}
    \mathrm{KL}(k) = D_{\mathrm{KL}}(p_k \| p_{k+1}) = \int p_k(\mathbf{w}) \log \frac{p_k(\mathbf{w})}{p_{k+1}(\mathbf{w})} \, d\mathbf{w} \leqslant \varepsilon.
    \end{equation}
\end{definition}

KL-дивергенция характеризует <<расстояние>> между распределениями: при близких апостериорах $\mathrm{KL}(k)$ мала.

\subsection{S-достаточность: метрика близости распределений}

В качестве альтернативы KL-дивергенции можно использовать функцию s-score~\cite{Aduenko2017}, задающую меру близости двух плотностей:
\begin{equation}
\label{eq:s-score}
\mathrm{s}\text{-score}(g_1, g_2) = \frac{\int g_1(\mathbf{w}) g_2(\mathbf{w}) \, d\mathbf{w}}{\max_{\mathbf{b}} \int g_1(\mathbf{w} - \mathbf{b}) g_2(\mathbf{w}) \, d\mathbf{w}}.
\end{equation}
Числитель~--- интеграл произведения плотностей; знаменатель нормирует значение. Для идентичных распределений s-score равно 1.

\begin{definition}
    Зафиксируем $\varepsilon > 0$. Размер выборки $m^*$ называется S-достаточным, если для всех $k \geqslant m^*$
    \begin{equation}
    \label{eq:S-sufficient}
    S(k) = \mathrm{s}\text{-score}(p_k, p_{k+1}) \geqslant 1 - \varepsilon.
    \end{equation}
\end{definition}

Таким образом, при достаточном размере выборки апостериоры на схожих подвыборках должны быть близки в смысле s-score.

\section{Сходимость оптимизационной поверхности в вероятностной модели линейной регрессии}\label{sec:linear-theory}

\begin{definition}
    Критерий достаточности называется \emph{корректным}, если при увеличении размера выборки соответствующая функция сходится к предельному значению.
\end{definition}

Отметим, что по сути корректность критерия позволяет гарантированно достичь заданного порога~$\varepsilon$, ограничивающего целевую функцию, при достаточно большом~$k$.

В этом разделе мы сперва формулируем результаты о сходимости распределений параметров модели в

\subsection{Сходимость распределения параметров модели}

Обозначим $\mathbb{E} \hat{\mathbf{w}}_k = \mathbf{m}_k$ и $\mathbb{D} \hat{\mathbf{w}}_k = \boldsymbol{\Sigma}_k$ (здесь $\mathbf{m}_k$, $\boldsymbol{\Sigma}_k$~--- моменты распределения оценки ММП; в байесовской постановке с сопряженным априором они совпадают с параметрами апостериора).

\begin{theorem}\label{th:M-sufficient}
    Пусть $\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и $\|\boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k\|_F \to 0$ при $k \to \infty$. Тогда в модели линейной регрессии определение M-достаточного размера выборки корректно: для любого $\varepsilon > 0$ существует $m^*$ такое, что для всех $k \geqslant m^*$ выполняется $M(k) \leqslant \varepsilon$.
\end{theorem}

Для моделей с нормальным апостериорным распределением параметров (в том числе линейной регрессии с гауссовским априором) KL-дивергенция и s-score допускают явные выражения через $\mathbf{m}_k$, $\mathbf{m}_{k+1}$, $\boldsymbol{\Sigma}_k$, $\boldsymbol{\Sigma}_{k+1}$. Благодаря этому формулируются две следующих теоремы.

\begin{theorem}\label{th:KL-sufficient}
    Пусть апостериорные распределения $p_k$, $p_{k+1}$ нормальны и $\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$, $\|\boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k\|_F \to 0$ при $k \to \infty$. Тогда определение KL-достаточного размера выборки корректно: для любого $\varepsilon > 0$ существует $m^*$ такое, что $\mathrm{KL}(k) \leqslant \varepsilon$ для всех $k \geqslant m^*$.
\end{theorem}

\begin{theorem}\label{th:S-sufficient}
    Пусть апостериорные распределения $p_k$, $p_{k+1}$ нормальны и $\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ при $k \to \infty$. Тогда определение S-достаточного размера выборки корректно: для любого $\varepsilon > 0$ существует $m^*$ такое, что $S(k) \geqslant 1 - \varepsilon$ для всех $k \geqslant m^*$.
\end{theorem}

Отметим, что для S-достаточности сходимости ковариационных матриц не требуется~--- достаточно сходимости средних.

Для применения теорем~\ref{th:M-sufficient}, \ref{th:KL-sufficient}, и \ref{th:S-sufficient} необходимо проверить выполнение условий на моменты. В линейной регрессии с гауссовским априором апостериор нормален, и моменты задаются явными формулами. Следующий результат обеспечивает сходимость в модели линейной регрессии.

\begin{theorem}\label{th:moments}
    Пусть множества значений признаков и целевой переменной ограничены: $\exists M \in \mathbb{R}: \|\mathbf{x}\|_2 \leqslant M$, $|y| \leqslant M$. Пусть минимальное собственное значение матрицы $\mathbf{X}_k^\top \mathbf{X}_k$ удовлетворяет условию $\lambda_{\min}(\mathbf{X}_k^\top \mathbf{X}_k) = \omega(\sqrt{k})$ при $k \to \infty$. Тогда в линейной регрессии с нормальным априорным распределением параметров $\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и $\|\boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k\|_F \to 0$ при $k \to \infty$.
\end{theorem}

Условие $\lambda_{\min}(\mathbf{X}_k^\top \mathbf{X}_k) = \omega(\sqrt{k})$ означает, что минимальное собственное значение растет быстрее $\sqrt{k}$ и гарантирует отсутствие сильной мультиколлинеарности при росте выборки. На практике это условие часто выполняется для <<хорошо обусловленных>> данных.

\subsection{Сходимость полного байесовского прогноза}

В предыдущем разделе было показано, что при увеличении используемого размера выборки в модели линейной регрессии наблюдается сходимость апостериорных распределений параметров на схожих подвыборках. Однако на практике нас больше интересует прогноз, который можно сделать на тестовой выборке, предварительно настроив модель на обучающей.

Пусть предварительно было произведено разбиение выборки $\mathfrak{D}_m$ на обучающую и тестовую, то есть
\[
\mathfrak{D}_m = \mathfrak{D}_{m_1}^{\text{train}} \sqcup \mathfrak{D}_{m_2}^{\text{test}},
\]
где $\mathfrak{D}_{m_1}^{\text{train}} = (\mathbf{X}_{\text{train}}, \mathbf{y}_{\text{train}})$ и $\mathfrak{D}_{m_2}^{\text{test}} = (\mathbf{X}_{\text{test}}, \mathbf{y}_{\text{test}})$.

Рассмотрим подвыборку $(\mathbf{X}_k, \mathbf{y}_k) \subset \mathfrak{D}_{m_1}^{\text{train}}$ и сформулируем теорему о близости полных байесовских прогнозов, сделанных на схожих подвыборках обучающей выборки.

\begin{theorem}\label{th:prediction-convergence}
Пусть множества значений признаков и целевой переменной ограничены, то есть $\exists M \in \mathbb{R}:$ $\|\mathbf{x}\|_2 \leqslant M$ и $|y| \leqslant M$. Если $\lambda_{\min}(\mathbf{X}_k^\top \mathbf{X}_k) = \omega(\sqrt{k})$ при $k \to \infty$, то в модели линейной регрессии с нормальным априорным распределением параметров
\[
D_{\mathrm{KL}}\left(p(\mathbf{y}_{\text{test}}|\mathbf{X}_{\text{test}}, \mathbf{X}_k, \mathbf{y}_k) \| p(\mathbf{y}_{\text{test}}|\mathbf{X}_{\text{test}}, \mathbf{X}_{k+1}, \mathbf{y}_{k+1})\right) \to 0
\]
при $k \to \infty$.
\end{theorem}

\section{Доказательства теоретических результатов}

В данном приводятся доказательства основных теорем настоящей главы: \ref{th:M-sufficient}, \ref{th:KL-sufficient}, \ref{th:S-sufficient}, \ref{th:moments} и \ref{th:prediction-convergence}. Начнем с доказательства теоремы~\ref{th:M-sufficient} о корректности определения M-достаточного размера выборки.

\begin{proof}[Доказательство теоремы~\ref{th:M-sufficient}]
    Рассмотрим определение M-достаточного размера выборки в терминах логарифма функции правдоподобия. В модели линейной регрессии
    \begin{align}
        L\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) &= p(\mathbf{y} | \mathbf{X}, \hat{\mathbf{w}}_k) = \prod_{i=1}^{m} p(y_i | \mathbf{x}_i, \hat{\mathbf{w}}_k) = \prod_{i=1}^{m} \mathcal{N}\left( y_i | \hat{\mathbf{w}}_k^{\top} \mathbf{x}_i, \sigma^2 \right) \notag \\
        &= \left(2\pi\sigma^2 \right)^{-m/2} \exp\left(-\frac{1}{2\sigma^2}\|\mathbf{y} -\mathbf{X} \hat{\mathbf{w}}_k\|_2^2 \right). \label{eq:proof:M-sufficient:1}
    \end{align}
    Логарифмируя~\eqref{eq:proof:M-sufficient:1}, получаем
    \begin{equation}
        l\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = \log p(\mathbf{y} | \mathbf{X}, \hat{\mathbf{w}}_k) = -\frac{m}{2}\log\left( 2\pi\sigma^2 \right) - \frac{1}{2\sigma^2} \| \mathbf{y} - \mathbf{X} \hat{\mathbf{w}}_k \|_2^2. \label{eq:proof:M-sufficient:2}
    \end{equation}
    Возьмем математическое ожидание~\eqref{eq:proof:M-sufficient:2} по $\mathfrak{D}_k$, учитывая, что $\mathbb{E}_{\mathfrak{D}_k}\hat{\mathbf{w}}_k=\mathbf{m}_k$ и $\text{cov}(\hat{\mathbf{w}}_k) = \mathbf{\Sigma}_k$, тогда имеем
    \begin{equation}
        \mathbb{E}_{\mathfrak{D}_k} l\left( \mathfrak{D}_m, \hat{\mathbf{w}}_k \right) = -\frac{m}{2}\log\left( 2\pi\sigma^2 \right) - \frac{1}{2\sigma^2} \Big( \| \mathbf{y} - \mathbf{X} \mathbf{m}_k \|_2^2 + \text{tr}\left( \mathbf{X}^{\top}\mathbf{X} \mathbf{\Sigma}_k \right) \Big). \label{eq:proof:M-sufficient:3}
    \end{equation}
    Используя~\eqref{eq:proof:M-sufficient:3} для размеров~$k$ и $k+1$, запишем выражение для разности математических ожиданий, в таком случае получим
    \begin{align}
        &\mathbb{E}_{\mathfrak{D}_{k+1}} l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k+1}) - \mathbb{E}_{\mathfrak{D}_k} l(\mathfrak{D}_m, \hat{\mathbf{w}}_{k}) = \notag \\
        &= \frac{1}{2\sigma^2} \Big( \| \mathbf{y} - \mathbf{X} \mathbf{m}_k \|_2^2 - \| \mathbf{y} - \mathbf{X} \mathbf{m}_{k+1} \|_2^2 \Big) + \frac{1}{2\sigma^2} \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big) \notag \\
        &= \frac{1}{\sigma^2} \mathbf{y}^{\top} \mathbf{X} (\mathbf{m}_{k+1} - \mathbf{m}_k) \label{eq:proof:M-sufficient:4} \\
        &+ \frac{1}{2\sigma^2} (\mathbf{m}_k - \mathbf{m}_{k+1})^{\top} \mathbf{X}^{\top}\mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \label{eq:proof:M-sufficient:5} \\
        &+ \frac{1}{2\sigma^2} \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big). \label{eq:proof:M-sufficient:6}
    \end{align}
    Значение функции $M(k)$ есть модуль от выражения выше. Применим неравенство треугольника для модуля, а затем оценим каждое слагаемое. Первое слагаемое~\eqref{eq:proof:M-sufficient:4} оценим, используя неравенство Коши--Буняковского:
    \begin{equation}
        \big| \mathbf{y}^{\top}\mathbf{X}(\mathbf{m}_{k+1}-\mathbf{m}_k)\big| \leqslant \| \mathbf{X}^{\top}\mathbf{y} \|_2 \|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2. \label{eq:proof:M-sufficient:7}
    \end{equation}
    Второе слагаемое~\eqref{eq:proof:M-sufficient:5} оценим, используя неравенство Коши--Буняковского, свойство согласованности спектральной матричной нормы, а также ограниченность последовательности векторов $\mathbf{w}_k$, которая следует из предъявленной в условии сходимости:
    \begin{align}
        \big| (\mathbf{m}_k - \mathbf{m}_{k+1})^{\top} \mathbf{X}^{\top}\mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \big| &\leqslant \| \mathbf{X} (\mathbf{m}_k - \mathbf{m}_{k+1}) \|_2 \| \mathbf{X} (\mathbf{m}_k + \mathbf{m}_{k+1}) \|_2 \notag \\
        &\leqslant \| \mathbf{X} \|_2^2 \| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2 \| \mathbf{m}_k + \mathbf{m}_{k+1} \|_2 \notag \\
        &\leqslant C \| \mathbf{X} \|_2^2 \| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2. \label{eq:proof:M-sufficient:8}
    \end{align}
    Последнее слагаемое~\eqref{eq:proof:M-sufficient:6} оценим, используя неравенство Гёльдера для нормы Фробениуса:
    \begin{equation}
        \Big| \text{tr} \Big( \mathbf{X}^{\top}\mathbf{X} \left( \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \right) \Big) \Big| \leqslant \| \mathbf{X}^{\top}\mathbf{X} \|_F \| \mathbf{\Sigma}_k - \mathbf{\Sigma}_{k+1} \|_F. \label{eq:proof:M-sufficient:9}
    \end{equation}
    Наконец, поскольку по условию теоремы $\| \mathbf{m}_k - \mathbf{m}_{k+1} \|_2 \to 0$ и $\| \boldsymbol{\Sigma}_k - \boldsymbol{\Sigma}_{k+1} \|_{F} \to 0$ при $k \to \infty$, то выражения~\eqref{eq:proof:M-sufficient:7}, \eqref{eq:proof:M-sufficient:8} и \eqref{eq:proof:M-sufficient:9} стремятся к 0 при $k \to \infty$, а следовательно и $M(k) \to 0$ при $k \to \infty$, что доказывает теорему.
\end{proof}

Теорема~\ref{th:M-sufficient} показывает, что при достаточном объеме выборки добавление очереднего объекта практически не изменяет ожидаемое значение функции правдоподобия на полной выборке. Тем не менее, особый интерес представляют результаты, полученные для полных апостериорных распределений параметров.

\begin{proof}[Доказательство теоремы~\ref{th:KL-sufficient}]
    Дивергенция Кульбака"--~Лейблера для пары нормальных апостериорных распределений имеет вид
    \begin{multline}
        D_{\text{KL}}\left( p_k \| p_{k+1} \right) = \frac{1}{2} \Bigg( \mathrm{tr}\left( \mathbf{\Sigma}_{k+1}^{-1} \mathbf{\Sigma}_k \right) + (\mathbf{m}_{k+1} - \mathbf{m}_k)^\top \mathbf{\Sigma}_{k+1}^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) - \\
        - n + \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} \Bigg). \label{eq:proof:KL-sufficient:1}
    \end{multline}
    Представим $\boldsymbol{\Sigma}_{k+1}$ как $\boldsymbol{\Sigma}_{k+1} = \boldsymbol{\Sigma}_k + \Delta \boldsymbol{\Sigma}$. Рассмотрим в отдельности каждое слагаемое в~\eqref{eq:proof:KL-sufficient:1}.
    Для первого слагаемого получаем
    \begin{equation}
        \mathrm{tr}\left( \mathbf{\Sigma}_{k+1}^{-1} \mathbf{\Sigma}_k \right) = \mathrm{tr}\left( \left( \mathbf{\Sigma}_k + \Delta \boldsymbol{\Sigma} \right)^{-1} \mathbf{\Sigma}_k \right) \to \mathrm{tr} \left( \mathbf{I}_n \right) = n \label{eq:proof:KL-sufficient:2}
    \end{equation}
    при $\| \Delta \boldsymbol{\Sigma} \|_F \to 0$. Для второго слагаемого имеем
    \begin{equation}
        \left| (\mathbf{m}_{k+1} - \mathbf{m}_k)^\top \mathbf{\Sigma}_{k+1}^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right| \leqslant \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2^2 \| \mathbf{\Sigma}_{k+1}^{-1} \|_2 \to 0 \label{eq:proof:KL-sufficient:3}
    \end{equation}
    при $ \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 \to 0$. Наконец, для последнего слагаемого можем записать
    \begin{equation}
        \log{\left( \frac{\det \mathbf{\Sigma}_{k+1}}{\det \mathbf{\Sigma}_{k}} \right)} = \log{\left( \frac{\det \left( \mathbf{\Sigma}_k + \Delta \boldsymbol{\Sigma} \right)}{\det \mathbf{\Sigma}_{k}} \right)} \to \log \det \mathbf{I}_n = \log 1 = 0 \label{eq:proof:KL-sufficient:4}
    \end{equation}
    при $ \| \Delta \boldsymbol{\Sigma} \|_F \to 0$. Объединяя предельные соотношения~\eqref{eq:proof:KL-sufficient:2}, \eqref{eq:proof:KL-sufficient:3} и \eqref{eq:proof:KL-sufficient:4}, получаем требуемое в условиях теоремы.
\end{proof}

\begin{proof}[Доказательство теоремы~\ref{th:S-sufficient}]
    Воспользуемся выражением s-score для пары нормальных априорных распределений из \cite{Aduenko2017}:
    \begin{equation}
        \text{s-score}(p_k, p_{k+1}) = \exp{\left( -\frac{1}{2} (\mathbf{m}_{k+1} - \mathbf{m}_k)^\top \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right)}. \label{eq:proof:S-sufficient:1}
    \end{equation}
    Поскольку
    \begin{multline}
        \left| (\mathbf{m}_{k+1} - \mathbf{m}_k)^\top \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} (\mathbf{m}_{k+1} - \mathbf{m}_k) \right| \leqslant \\
        \leqslant \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2^2 \| \left( \mathbf{\Sigma}_k + \mathbf{\Sigma}_{k+1} \right)^{-1} \|_2 \to 0
    \end{multline}
    при $\| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 \to 0$, то значение квадратичной формы внутри экспоненты~\eqref{eq:proof:S-sufficient:1} стремится к нулю. Следовательно, $\text{s-score}(p_k, p_{k+1}) \to 1$ при устремлении $\| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 \to 0$, что и требовалось доказать.
\end{proof}

В вероятностной модели линейной регрессии с гауссовским априорным распределением доказывается следующая теорема, определяющая достаточные условия для сходимостей из предыдущих теорем~\ref{th:KL-sufficient} и \ref{th:S-sufficient}.

\begin{proof}[Доказательство теоремы~\ref{th:moments}]
    Пусть задано нормальное априорное распределение параметров $p(\mathbf{w}) = \mathcal{N}\left( \mathbf{w} | \mathbf{0}, \alpha^{-1} \mathbf{I} \right)$. В модели линейной регрессии правдоподобие является нормальным, а именно
    \begin{equation}
        p(\mathbf{y} | \mathbf{X}, \mathbf{w}) = \mathcal{N}\left( \mathbf{y} | \mathbf{X} \mathbf{w}, \sigma^2 \mathbf{I} \right) = \left( 2\pi\sigma^2 \right)^{-m/2} \exp\left( -\frac{1}{2\sigma^2} \| \mathbf{y} - \mathbf{X} \mathbf{w} \|_2^2 \right). \label{eq:proof:moments:1}
    \end{equation}
    Используя сопряженность априорного распределения и правдоподобия, легко найти параметры апостериорного распределения:
    \begin{equation*}
        p(\mathbf{w} | \mathbf{X}, \mathbf{y}) = \mathcal{N}\left( \mathbf{w} | \mathbf{m}, \boldsymbol{\Sigma} \right),
    \end{equation*}
    где
    \begin{equation}
        \boldsymbol{\Sigma} = \left( \alpha \mathbf{I} + \frac{1}{\sigma^2} \mathbf{X}^\top \mathbf{X} \right)^{-1}, \qquad \mathbf{m} = \left( \mathbf{X}^\top \mathbf{X} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}^\top \mathbf{y}.
        \label{eq:proof:moments:2}
    \end{equation}
    Рассмотрим выражение $\| \boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k \|_2$ нормы разности матриц ковариции для подвыборок размера $k$ и $k+1$. Введем обозначение $\mathbf{A}_k = \frac{1}{\sigma^2} \mathbf{X}^\top_k \mathbf{X}_k$. Учитывая формулы моментов~\eqref{eq:proof:moments:2}, имеем
    \begin{align}
        &\| \boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k \|_2 = \notag \\
        &= \left\| \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} - \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} \right\|_2 \notag \\
        &= \left\| \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} \left( \mathbf{A}_{k+1} - \mathbf{A}_k \right) \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} \right\|_2 \notag \\
        &\leqslant \left\| \left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)^{-1} \right\|_2 \left\| \left( \alpha \mathbf{I} + \mathbf{A}_k \right)^{-1} \right\|_2 \left\| \mathbf{A}_{k+1} - \mathbf{A}_k \right\|_2 \label{eq:proof:moments:3} \\
        &= \frac{1}{\lambda_{\min}\left( \alpha \mathbf{I} + \mathbf{A}_{k+1} \right)} \frac{1}{\lambda_{\min}\left( \alpha \mathbf{I} + \mathbf{A}_k \right)} \left\| \mathbf{A}_{k+1} - \mathbf{A}_k \right\|_2 \notag \\
        &\leqslant \frac{1}{\lambda_{\min}\left( \mathbf{A}_{k+1} \right)} \frac{1}{\lambda_{\min}\left( \mathbf{A}_k \right)} \left\| \mathbf{A}_{k+1} - \mathbf{A}_k \right\|_2 \notag \\
        &= \sigma^2  \frac{1}{\lambda_{\min}\left( \mathbf{X}^\top_{k+1} \mathbf{X}_{k+1} \right)} \frac{1}{\lambda_{\min}\left( \mathbf{X}^\top_k \mathbf{X}_k \right)} \left\| \mathbf{X}^\top_{k+1} \mathbf{X}_{k+1} - \mathbf{X}^\top_k \mathbf{X}_k \right\|_2. \label{eq:proof:moments:4}
    \end{align}
    Здесь в~\eqref{eq:proof:moments:3} мы воспользовались субмультипликативностью спектральной матричной нормы, а в \eqref{eq:proof:moments:3} "--- выражением спектральной матричной нормы через максимальное собственное значение.
    Далее, поскольку по условию $\| \mathbf{x} \|_2 \leqslant M$, то мы оцениваем норму разности матриц из~\eqref{eq:proof:moments:4} следующим образом:
    \begin{align}
        \left\| \mathbf{X}^\top_{k+1} \mathbf{X}_{k+1} - \mathbf{X}^\top_k \mathbf{X}_k \right\|_2 
        &= \left\| \sum\limits_{i=1}^{k+1} \mathbf{x}_i \mathbf{x}_i^\top - \sum\limits_{i=1}^{k} \mathbf{x}_i \mathbf{x}_i^\top \right\|_2 = \left\| \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right\|_2 \notag \\
        &= \lambda_{\max}\left( \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)  = \mathbf{x}_{k+1}^\top \mathbf{x}_{k+1} = \| \mathbf{x}_{k+1} \|_2^2 \leqslant M^2, \label{eq:proof:moments:4}
    \end{align}
    где переход к~\eqref{eq:proof:moments:4} имеет место потому, что матрица единичного ранга имеет единственное ненулевое собственное значение. По условию $\lambda_{\min}\left( \mathbf{X}^\top_k \mathbf{X}_k \right) = \omega(\sqrt{k})$, тогда $\| \boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k \|_2 = o(k^{-1})$ при $k \to \infty$. Пользуясь эквивалентностью матричных норм, получаем требуемую сходимость для матриц ковариации:
    \begin{equation}
        \| \boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k \|_F \leqslant \sqrt{k} \| \boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k \|_2 = o(k^{-1/2}), \quad k \to \infty.
    \end{equation}
    Теперь оценим норму разности математических ожиданий. Пользуясь формулой~\eqref{eq:proof:moments:2}, получаем
    \begin{align}
        &\| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2 = \notag \\
        &= \left\| \left( \mathbf{X}_{k+1}^\top \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_{k+1}^\top \mathbf{y}_{k+1} - \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^\top \mathbf{y}_k \right\|_2 \notag \\
        &= \Big\| \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} + \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)^{-1} \left( \mathbf{X}_k^\top \mathbf{y}_k + \mathbf{x}_{k+1} y_{k+1} \right) \notag \\
        &\qquad\qquad\qquad\qquad\qquad\qquad\qquad - \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^\top \mathbf{y}_k \Big\|_2 \notag \\
        &= \Bigg\| \left[ \left( \mathbf{I} + \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)^{-1} - \mathbf{I} \right] \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{X}_k^\top \mathbf{y}_k \notag \\
        &\qquad\qquad\qquad\qquad\qquad\qquad\qquad + \left( \mathbf{X}_{k+1}^\top \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} y_{k+1} \Bigg\|_2 \label{eq:proof:moments:3} \\
        &\leqslant \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)^{-1} - \mathbf{I} \right\|_2 \left\| \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{X}_k^\top \mathbf{y}_k \right\|_2 \notag \\
        &\qquad\qquad\qquad\qquad\qquad\qquad\qquad + \left\| \left( \mathbf{X}_{k+1}^\top \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{x}_{k+1} y_{k+1} \right\|_2 \label{eq:proof:moments:4}
    \end{align}
    Здесь мы учли, что $\mathbf{X}_{k+1}^\top = [\mathbf{X}_k^\top, \mathbf{x}_{k+1}]$ и $\mathbf{y}_{k+1} = [\mathbf{y}_k, y_{k+1}]^\top$, тогда $\mathbf{X}_{k+1}^\top \mathbf{X}_{k+1} = \mathbf{X}_k^\top \mathbf{X}_k + \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top$ и $\mathbf{X}_{k+1}^\top \mathbf{y}_{k+1} = \mathbf{X}_k^\top \mathbf{y}_k + \mathbf{x}_{k+1} y_{k+1}$.
    Для перехода к~\eqref{eq:proof:moments:3} мы вынесли множитель в первом слагаемом, после чего вынесли общий множитель у обоих слагаемых. Далее для получения~\eqref{eq:proof:moments:4} мы  применили неравенство треугольника, а также свойства согласованности и субмультипликативности спектральной нормы. Оценим по отдельности каждое слагаемое в~\eqref{eq:proof:moments:4}. В первом множителе первого слагаемого применим формулу для разности обратных матриц, как мы делали с ковариационными матрицами:
    \begin{align}
        & \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)^{-1} - \mathbf{I} \right\|_2 \leqslant \notag \\
        &\leqslant \left\| \left( \mathbf{I} + \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)^{-1} \right\|_2 \left\| \mathbf{I} \right\|_2 \left\| \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right\|_2 \leqslant \notag \\
        &\leqslant \frac{1}{\lambda_{\min}\left( \mathbf{I} + \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)} \frac{\left\| \mathbf{x}_{k+1} \right\|_2^2}{\lambda_{\min}\left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)} \label{eq:proof:moments:5} \\
        &\leqslant \frac{1}{1 + \lambda_{\min}\left(\left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)} \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_k^\top \mathbf{X}_k \right)} \notag \\
        & \leqslant \frac{1}{1 + \lambda_{\max}\left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right) \lambda_{\min}\left( \mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top \right)} \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_k^\top \mathbf{X}_k \right)} \notag \\
        &= \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_k^\top \mathbf{X}_k \right)}. \label{eq:proof:moments:6}
    \end{align}

    Выше, при переходе к~\eqref{eq:proof:moments:5}, мы снова использовали субмультипликативность, а также выражение для нормы матрицы единичного ранга. Наконец, для получения~\eqref{eq:proof:moments:6} мы применили, что минимальное собственное значение произведения матриц оценивается произведением их минимальных собственных значений, а минимальное собственное значение матрицы единичного ранга $\mathbf{x}_{k+1} \mathbf{x}_{k+1}^\top$ равно нулю. Второй и третий множители первого слагаемого оцениваются следующим образом:
    \begin{align}
        \left\| \left( \mathbf{X}_k^\top \mathbf{X}_k + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{X}_k^\top \mathbf{y}_k \right\|_2
        &\leqslant \frac{\left\| \mathbf{X}_k^\top \mathbf{y}_k \right\|_2}{\lambda_{\min}\left( \mathbf{X}_k^\top \mathbf{X}_k \right)} = \frac{\left\| \sum\limits_{i=1}^{k} \mathbf{x}_i y_i \right\|_2}{\lambda_{\min}\left( \mathbf{X}_k^\top \mathbf{X}_k \right)} \notag \\
        &\leqslant \frac{k M^2}{\lambda_{\min}\left( \mathbf{X}_k^\top \mathbf{X}_k \right)}. \label{eq:proof:moments:7}
    \end{align}
    Наконец, оценим второе слагаемое.
    \begin{equation}
        \left\| \left( \mathbf{X}_{k+1}^\top \mathbf{X}_{k+1} + \alpha \sigma^2 \mathbf{I} \right)^{-1} \right\|_2 \left\| \mathbf{x}_{k+1} y_{k+1} \right\|_2 \leqslant \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_{k+1}^\top \mathbf{X}_{k+1} \right)} \label{eq:proof:moments:8}
    \end{equation}
    Итого, собирая воедино~\eqref{eq:proof:moments:6}, \eqref{eq:proof:moments:7} и \eqref{eq:proof:moments:8}, имеем следующую оценку:
    \begin{align}
        \| \mathbf{m}_{k+1} - \mathbf{m}_k \|_2
        &\leqslant \frac{k M^3}{\lambda_{\min}^2\left( \mathbf{X}_k^\top \mathbf{X}_k \right)} + \frac{M^2}{\lambda_{\min}\left( \mathbf{X}_{k+1}^\top \mathbf{X}_{k+1} \right)} \notag \\
        &= k \cdot o(k^{-1}) + o(k^{-1/2}) = o(1)
    \end{align}
    при $k \to \infty$. Таким образом, получили требуемую сходимость.
\end{proof}

Наконец, докажем теорему о сходимости полного байесовского прогноза на отложенной выборке.

\begin{proof}[Доказательство теоремы~\ref{th:prediction-convergence}]
    Полный байесовский прогноз для одиночной модели выражается по формуле
    \begin{align}
        p(\mathbf{y}_{\text{test}}|\mathbf{X}_{\text{test}}, \mathbf{X}_k, \mathbf{y}_k)
        &= \int p(\mathbf{y}_{\text{test}}|\mathbf{X}_{\text{test}}, \mathbf{w}) p(\mathbf{w}|\mathbf{X}_k, \mathbf{y}_k) d\mathbf{w} \notag \\
        &= \int \mathcal{N}(\mathbf{y}_{\text{test}} | \mathbf{X}_{\text{test}}\mathbf{w}, \sigma^2 \mathbf{I}) \mathcal{N}(\mathbf{w} | \mathbf{m}_k, \boldsymbol{\Sigma}_k) d\mathbf{w} \label{eq:proof:prediction-convergence:1} \\
        &= \mathcal{N}(\mathbf{y}_{\text{test}} | \mathbf{X}_{\text{test}} \mathbf{m}_k, \sigma^2 \mathbf{I} + \mathbf{X}_{\text{test}} \boldsymbol{\Sigma}_k \mathbf{X}_{\text{test}}^\top). \label{eq:proof:prediction-convergence:2}
    \end{align}
    Здесь при переходе от~\eqref{eq:proof:prediction-convergence:1} к~\eqref{eq:proof:prediction-convergence:2} мы воспользовались известным выражением из~\cite{bishop2006pattern}. В условиях настоящей теоремы справедливы результаты теоремы~\ref{th:moments}. Следовательно, $\|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0$ и $\|\boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_k\|_F \to 0$ при $k \to \infty$. Далее, остается показать сходимость математического ожидания и ковариационной матрицы нормального распределения прогноза модели. Для математического ожидания при $k \to \infty$ выполнено
    \begin{equation}
        \|\mathbf{X}_{\text{test}} \mathbf{m}_{k+1} - \mathbf{X}_{\text{test}} \mathbf{m}_{k}\|_2 \leqslant \|\mathbf{X}_{\text{test}}\|_2 \|\mathbf{m}_{k+1} - \mathbf{m}_k\|_2 \to 0.
        \label{eq:proof:prediction-convergence:3}
    \end{equation}
    Для ковариационной матрицы, ввиду субмультипликативности нормы Фробениуса, при $k \to \infty$ справедливо следующее:
    \begin{equation}
        \|\sigma^2 \mathbf{I} + \mathbf{X}_{\text{test}} \boldsymbol{\Sigma}_{k+1} \mathbf{X}_{\text{test}}^\top - \sigma^2 \mathbf{I} - \mathbf{X}_{\text{test}} \boldsymbol{\Sigma}_{k} \mathbf{X}_{\text{test}}^\top\|_F \leqslant \|\mathbf{X}_{\text{test}}\|_F^2 \|\boldsymbol{\Sigma}_{k+1} - \boldsymbol{\Sigma}_{k}\|_F \to 0.
        \label{eq:proof:prediction-convergence:4}
    \end{equation}
    Наконец, имея сходимость математического ожидания~\eqref{eq:proof:prediction-convergence:3} и ковариационной матрицы~\eqref{eq:proof:prediction-convergence:4}, действуем аналогично доказательству теоремы~\ref{th:KL-sufficient} и получаем, что
    \begin{equation*}
        D_{\mathrm{KL}}\left(p(\mathbf{y}_{\text{test}}|\mathbf{X}_{\text{test}}, \mathbf{X}_k, \mathbf{y}_k) \| p(\mathbf{y}_{\text{test}}|\mathbf{X}_{\text{test}}, \mathbf{X}_{k+1}, \mathbf{y}_{k+1})\right) \to 0
    \end{equation*}
    при $k \to \infty$, завершая доказательство данной теоремы.
\end{proof}

\section{Вычислительные эксперименты}

TODO

\section{Обсуждение}

\textbf{Логистическая регрессия.} В задаче бинарной классификации с метками $y \in \{0, 1\}$ логистическая регрессия задается правдоподобием
\begin{equation}
p(y | \mathbf{x}, \mathbf{w}) = \sigma(\mathbf{w}^\top \mathbf{x})^y (1 - \sigma(\mathbf{w}^\top \mathbf{x}))^{1-y}, \qquad \sigma(z) = \frac{1}{1 + e^{-z}}.
\end{equation}
Апостериорное распределение параметров уже не является нормальным; сопряженного априора не существует. Тем не менее при достаточно большом размере выборки апостериор асимптотически нормален (результат Бернштейна"--~фон Мизеса). На практике критерии KL и S применяются к логистической регрессии, а порог достаточности выбирается эмпирически~\cite{MOTRENKO2014743}. Теоретическое обоснование сходимости для логистической регрессии требует дополнительного анализа.

\textbf{Обобщенные линейные модели (GLM).} Обобщенные линейные модели включают линейную и логистическую регрессию как частные случаи~\cite{mccullagh1989generalized}. Модель задается линейным предиктором $\eta = \mathbf{w}^\top \mathbf{x}$, функцией связи и распределением из экспоненциального семейства. Для GLM с канонической функцией связи оценка ММП существует при определенных условиях на данные. Сходимость апостериора и применимость критериев D, M, KL, S зависят от конкретного вида модели; для ряда GLM (например, регрессии Пуассона) возможны аналоги теорем 2.1"--~2.4 при дополнительных предположениях~\cite{Grabovoy2022}.

\textbf{Сравнительный анализ критериев.} Критерии D и M основаны на бутстрапировании подвыборок и вычислении правдоподобия в точке ММП. Их преимущество~--- простота вычислений: не требуется полное апостериорное распределение. Критерии KL и S требуют знания апостериора на схожих подвыборках; для линейной регрессии с сопряженным априором они вычисляются аналитически.

Эмпирические исследования показывают, что KL-достаточный размер, как правило, оказывается наибольшим, а S-достаточный~--- наименьшим при фиксированном пороге $\varepsilon$. Это объясняется различной чувствительностью KL-дивергенции и s-score к различиям распределений. Выбор критерия зависит от задачи: при необходимости консервативной оценки предпочтителен KL; при ограничениях на объем данных может быть достаточен критерий S.