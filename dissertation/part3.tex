\chapter{Стабилизация оптимизационной поверхности в нейронных сетях}\label{ch:landscapes}

В данной главе рассматриваются методы определения достаточного размера обучающей выборки для матрично-представимых нейронных сетей на основе анализа стабилизации оптимизационной поверхности (ландшафта функции потерь) при увеличении объема данных. Сначала обсуждается переход от линейных моделей к нейросетевым и специфика последних: невыпуклость функции потерь, множественные минимумы, симметрии в пространстве параметров. Далее вводится матрица Гессе как инструмент анализа геометрии поверхности потерь и формулируется связь между сходимостью поверхности и спектральными свойствами Гессе. Затем приводятся теоретические оценки спектральной нормы матрицы Гессе для различных архитектур (полносвязные, сверточные сети, трансформеры) и теорема о сходимости поверхности потерь. Полученные результаты дают теоретическое обоснование эмпирических законов масштабирования и позволяют количественно оценить достаточный размер выборки для заданной архитектуры сети.

\section{Введение}

Методы определения достаточного размера выборки, разработанные для линейных моделей (глава~\ref{ch:linear}), опираются на свойства правдоподобия и апостериорных распределений параметров. Для вероятностной линейной регрессии с гауссовским априором апостериорное распределение имеет аналитический вид, что позволяет получить явные оценки сходимости моментов и дивергенций между распределениями на подвыборках.

Однако для нейронных сетей применение этих методов сталкивается с принципиальными ограничениями. Во-первых, апостериорное распределение параметров нейросети не имеет аналитического выражения даже при гауссовском априоре: правдоподобие нелинейно по параметрам, и сопряженного априора не существует. Во-вторых, многократное полное переобучение сети на подвыборках разного размера для построения кривой обучения вычислительно дорого для больших моделей. В-третьих, классические статистические критерии, ориентированные на единственную оценку параметров, плохо применимы к нейросетям из-за невыпуклости функции потерь и наличия множественных эквивалентных минимумов.

В то же время эмпирические законы масштабирования показывают устойчивые зависимости качества моделей от размера данных и числа параметров~\cite{kaplan2020scaling,hoffmann2022chinchila}. Эти результаты опираются на масштабные эксперименты и не имеют пока единого теоретического обоснования, связывающего геометрию оптимизационной задачи с объемом выборки. Таким образом, возникает необходимость в новых подходах, опирающихся на объекты, доступные для анализа в нейросетях~--- например, на поверхность функции потерь и ее геометрические характеристики, задаваемые матрицей Гессе.

\section{Геометрия оптимизационной поверхности в нейронных сетях}

Ландшафтом (оптимизационной поверхностью) функции потерь называют график $\mathcal{L}_m(\mathbf{w})$ как функции от $\mathbf{w} \in \mathbb{R}^P$, где $P$~--- число параметров модели. Для нейронных сетей эта поверхность имеет сложную структуру: множественные локальные минимумы, седловые точки, плато~\cite{li2018visualizing,fort2019large}.

\subsection{Структура множества оптимальных параметров}

В отличие от линейной регрессии, где при невырожденной матрице признаков существует единственный глобальный минимум, функция потерь нейронной сети, как правило, невыпукла. Более того, из-за симметрий (перестановка нейронов, знаковые преобразования) многие минимумы эквивалентны с точки зрения предсказательной способности~\cite{draxler2018essentially}. В работе~\cite{garipov2018loss} показано, что множество решений связно: можно перейти от одного минимума к другому без существенного роста потерь.

Плоские минимумы (с малой кривизной в окрестности) часто связывают с лучшей обобщающей способностью~\cite{chaudhari2019entropy}. Кривизна поверхности в окрестности минимума описывается матрицей Гессе $\mathbf{H} = \nabla^2_{\mathbf{w}} \mathcal{L}_m(\mathbf{w})$. Спектр Гессе характеризует жесткость минимума: большие собственные значения соответствуют <<острым>> направлениям, малые~--- <<плоским>>. Для типичных нейросетей наблюдается характерная структура спектра: множество малых собственных значений и небольшое число выбросов~\cite{papyan2019spectrum}.

\subsection{Стабилизация поверхности при увеличении выборки}

При увеличении размера выборки $m$ эмпирическая функция потерь $\mathcal{L}_m(\mathbf{w})$ сходится к популяционной $\mathcal{L}(\mathbf{w}) = \mathbb{E}_{(\mathbf{x},y) \sim p}[\ell(f_{\mathbf{w}}(\mathbf{x}), y)]$. Естественно ожидать, что сама поверхность <<стабилизируется>>: разность $\mathcal{L}_{m+1}(\mathbf{w}) - \mathcal{L}_m(\mathbf{w})$ уменьшается при росте $m$, особенно в окрестности минимума. Такая стабилизация может служить признаком достаточности выборки: если добавление нового объекта почти не меняет поверхность потерь в области решений, то выборка, по-видимому, уже достаточно представительна.

Теоретический анализ этой идеи требует оценки скорости сходимости разности потерь и связи с матрицей Гессе. В~\cite{kiselev2024unraveling} для полносвязных сетей получены верхние оценки разности $\mathcal{L}_{m+1}(\mathbf{w}) - \mathcal{L}_m(\mathbf{w})$ через спектральную норму Гессе, что дает зависимость от архитектуры сети и обратную зависимость от $m$. Эти результаты обобщены на сверточные архитектуры~\cite{meshkov2024convnets} и позволяют теоретически обосновать использование сходимости поверхности потерь как признака достаточного размера выборки.

\section{Матрица Гессе и разложение Гаусса"--~Ньютона}

Матрица Гессе $\mathbf{H}(\mathbf{w}) = \nabla^2_{\mathbf{w}} \mathcal{L}_m(\mathbf{w})$ второго порядка характеризует локальную кривизну поверхности потерь в точке $\mathbf{w}$. Ее спектральные свойства (собственные значения и векторы) определяют геометрию окрестности минимума и используются при анализе обобщения и сходимости оптимизации~\cite{papyan2019spectrum}.

\subsection{Разложение Гессе на G-терм и H-терм}

Для анализа сходимости поверхности потерь при увеличении размера выборки используется разложение функции потерь в окрестности минимума. Пусть $\mathbf{w}^*$~--- точка минимума функции $\mathcal{L}_m(\mathbf{w})$. Рассмотрим разность $\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})$ для подвыборок размера $k$ и $k+1$.

Используя правило цепочки для вычисления второй производной композиции функций, Гессиан функции потерь можно разложить на два слагаемых~\cite{sagun2018empiricalanalysishessianoverparametrized}:
\begin{equation}
\label{eq:hessian-decomposition}
\mathbf{H}_i(\mathbf{w}) = \underbrace{\nabla_{\mathbf{w}} \mathbf{z}_i \frac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_i^2} \nabla_{\mathbf{w}} \mathbf{z}_i^\top}_{\text{G-терм}} + \underbrace{\sum_{k=1}^K \frac{\partial \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial z_{ik}} \nabla^2_{\mathbf{w}} z_{ik}}_{\text{H-терм}},
\end{equation}
где $\mathbf{z}_i = f_{\mathbf{w}}(\mathbf{x}_i)$~--- выход сети (логиты), $\nabla_{\mathbf{w}} \mathbf{z}_i$~--- якобиан сети, $\frac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_i^2}$~--- Гессиан функции потерь по логитам.

G-терм (outer-product term) соответствует разложению Гаусса"--~Ньютона и доминирует в спектре Гессе: именно он дает выбросы (outliers) в спектре. H-терм (functional term) дает основную массу собственных значений около нуля (bulk)~\cite{papyan2019spectrumdeepnethessiansscale}. В окрестности минимума, где градиент потерь близок к нулю, H-терм пренебрежимо мал, и можно использовать приближение
\begin{equation}
\label{eq:hessian-approximation}
\mathbf{H}_i(\mathbf{w}) \approx \nabla_{\mathbf{w}} \mathbf{z}_i \frac{\partial^2 \ell(\mathbf{z}_i, \mathbf{y}_i)}{\partial \mathbf{z}_i^2} \nabla_{\mathbf{w}} \mathbf{z}_i^\top.
\end{equation}

\subsection{Связь сходимости поверхности со спектром Гессе}

Используя разложение Тейлора второго порядка в окрестности минимума $\mathbf{w}^*$ и приближение \eqref{eq:hessian-approximation}, разность потерь можно оценить через спектральную норму Гессе. Для подвыборок размера $k$ и $k+1$ выполняется:
\begin{multline}
    \label{eq:loss-difference-bound}
    \left|\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})\right| \leqslant \frac{1}{k+1}\left|\ell(f_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_k(\mathbf{w}^*)\right| + \\ + \frac{1}{k+1}\|\mathbf{w} - \mathbf{w}^*\|_2^2 \left\|\mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k}\sum_{i=1}^k \mathbf{H}_i(\mathbf{w}^*)\right\|_2.
\end{multline}

Первое слагаемое ограничено константой при ограниченности функции потерь. Второе слагаемое зависит от спектральной нормы разности Гессианов, которая, в свою очередь, оценивается через спектральную норму самого Гессиана. Таким образом, задача сводится к оценке спектральной нормы $\|\mathbf{H}_i(\mathbf{w})\|_2$ для различных архитектур нейронных сетей.

\section{Оценки спектральной нормы Гессе для полносвязных сетей}

Для полносвязной сети с $L$ слоями, где $l$-й слой имеет размер $d_l$, в работе~\cite{kiselev2024unraveling} получена оценка спектральной нормы Гессе через параметры архитектуры.

Рассмотрим $L$-слойную полносвязную сеть с ReLU-активацией, решающую задачу $K$-классовой классификации с кросс-энтропийной функцией потерь. Выход сети (логиты) вычисляется рекуррентно:
\begin{align*}
\mathbf{z}^{(p)} &= \mathbf{W}^{(p)} \mathbf{x}^{(p)} + \mathbf{b}^{(p)}, \\
\mathbf{x}^{(p+1)} &= \sigma(\mathbf{z}^{(p)}),
\end{align*}
где $\sigma$~--- функция ReLU, $\mathbf{x}^{(1)} = \mathbf{x}$~--- вход, $\mathbf{z} = f_{\mathbf{w}}(\mathbf{x}) = \mathbf{z}^{(L)}$~--- выход (логиты).

Используя матричное представление якобиана через произведения матриц весов и диагональных матриц активаций, можно получить оценку спектральной нормы Гессе~\cite{kiselev2024unraveling}.

\begin{theorem}\label{th:mlp-hessian}
Пусть $\mathcal{L}_m(\mathbf{w})$~--- функция потерь $L$-слойной полносвязной сети с ReLU-активацией без смещений, применяемой для решения задачи $K$-классовой классификации. Предположим, что выполнены ограничения: $\|\mathbf{W}^{(p)}\|_2 \leqslant M_{\mathbf{W}}$ для всех слоев $p = 1, \ldots, L$ и $\|\mathbf{x}_i\|_2 \leqslant M_{\mathbf{x}}$ для всех объектов $i = 1, \ldots, m$ в выборке. Тогда для любого объекта $i$ выполняется неравенство:
\begin{equation}
\label{eq:mlp-hessian-bound}
\|\mathbf{H}_i(\mathbf{w})\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 M_{\mathbf{W}}^{2L} + \sqrt{2} \frac{M_{\mathbf{W}}^2 (M_{\mathbf{W}}^{2L} - 1)}{M_{\mathbf{W}}^2 - 1}.
\end{equation}
\end{theorem}

Эта оценка показывает зависимость спектральной нормы Гессе от структуры сети. Если каждый параметр ограничен константой $M > 0$ (т.\,е. $|w_{ij}^{(p)}| \leqslant M$ для всех $i, j$ и слоев $p$), и скрытые слои имеют размер $h$, то оценка упрощается:

\begin{corollary}\label{cor:mlp-hessian-size}
При условиях теоремы~\ref{th:mlp-hessian} и ограничении параметров $|w_{ij}^{(p)}| \leqslant M$ для всех $i, j = 1, \ldots, h$ и слоев $p = 1, \ldots, L$ выполняется:
\begin{equation}
\|\mathbf{H}_i(\mathbf{w})\|_2 \leqslant L \sqrt{2} M_{\mathbf{x}}^2 (hM)^{2L} + \sqrt{2} \frac{(hM)^2 ((hM)^{2L} - 1)}{(hM)^2 - 1},
\end{equation}
откуда следует пропорциональность:
\begin{equation}
\|\mathbf{H}_i(\mathbf{w})\|_2 \propto L (hM)^{2L}.
\end{equation}
\end{corollary}

Таким образом, спектральная норма Гессе является степенной функцией размера скрытого слоя $h$ и экспоненциальной функцией числа слоев $L$. Хотя оценка может показаться завышенной, на практике при большом $h$ константа $M$, ограничивающая величину весов, обычно мала, что компенсирует рост оценки.

\section{Оценки спектральной нормы Гессе для сверточных сетей}

Для сверточных сетей оценка спектральной нормы Гессе учитывает параметры сверток и пулинга~\cite{meshkov2024convnets}. Ключевая идея~--- представление сверточных операций через матрицы Тёплица и использование матричного представления сети как произведения линейных операторов.

\subsection{Матричное представление сверточных сетей}

Сверточную сеть можно представить как композицию линейных операторов (сверток) и нелинейных активаций. Для $L$-слойной сверточной сети с ReLU-активацией:
\[
f_{\mathbf{w}}(\mathbf{x}) = \mathbf{T}^{(L+1)} \circ \sigma \circ \ldots \circ \sigma \circ \mathbf{T}^{(1)}(\mathbf{x}),
\]
где $\mathbf{T}^{(p)}$~--- линейный оператор свертки, $\sigma$~--- ReLU-активация. Используя диагональные матрицы активаций $\boldsymbol{\Lambda}^{(p)} = \text{diag}([\mathbf{z}^{(p)} \geqslant \mathbf{0}])$, сеть можно записать в виде произведения матриц:
\begin{equation}
\label{eq:conv-matrix-repr}
f_{\mathbf{w}}(\mathbf{x}) = \mathbf{T}^{(L+1)} \boldsymbol{\Lambda}^{(L)} \ldots \boldsymbol{\Lambda}^{(1)} \mathbf{T}^{(1)} \mathbf{x}.
\end{equation}

Для сверточных слоев матрицы $\mathbf{T}^{(p)}$ представляются через матрицы Тёплица~\cite{singh2023hessianperspectivenatureconvolutional}, что позволяет оценить норму якобиана и, следовательно, спектральную норму Гессе.

\subsection{Оценки для одномерных сверток}

Для одномерных сверточных сетей получена следующая оценка~\cite{meshkov2024convnets}:

\begin{theorem}\label{th:1d-conv-hessian}
Рассмотрим сеть $f_{\mathbf{w}}(\mathbf{x}) = C_{\mathbf{W}^{(L+1)}} \circ \sigma \circ \ldots \circ \sigma \circ C_{\mathbf{W}^{(1)}}$, где $C_{\mathbf{W}^{(i)}}$~--- одномерная свертка с ядром $\mathbf{W}^{(i)}$ без паддинга и со страйдом 1. Пусть заданы верхние границы: $C_l \leqslant C$ (число каналов), $k_i \leqslant k$ (размер ядра), $d_i \leqslant d_1 := d$ (размер последовательности), $|\mathbf{W}^{(p)}_{i,j,k}|^2 \leqslant w^2$ (норма весов). Тогда спектральная норма outer-product Гессиана оценивается как:
\begin{equation}
\label{eq:1d-conv-hessian-bound}
\|\mathbf{H}_O\|_2 \leqslant \sqrt{2} \|\mathbf{x}\|_2^2 d^2 (L+1) (C^2 w^2 k d)^L.
\end{equation}
\end{theorem}

\subsection{Оценки для двумерных сверток}

Для двумерных сверточных сетей, применяемых к изображениям, оценка учитывает размеры изображения и ядер свертки:

\begin{theorem}\label{th:2d-conv-hessian}
Рассмотрим сеть $f_{\mathbf{w}}(\mathbf{x}) = C_{\mathbf{W}^{(L+1)}} \circ \ldots \circ C_{\mathbf{W}^{(1)}}$, где $C_{\mathbf{W}^{(l)}}$~--- двумерная свертка с ядром $\mathbf{W}^{(l)}$ без паддинга и со страйдом 1. Пусть входное изображение имеет размеры $m \times n$ с $C$ каналами, размеры ядер ограничены $k_i \leqslant k$, и $|\mathbf{W}^{(p)}_{i,j,k}|^2 \leqslant w^2$. Тогда спектральная норма outer-product Гессиана оценивается как:
\begin{equation}
\label{eq:2d-conv-hessian-bound}
\|\mathbf{H}_O\|_2 \leqslant \sqrt{2} \|\mathbf{x}\|_2^2 q^2 (L+1) (C^2 k^2 w^2 m n)^L,
\end{equation}
где $q^2 = C^2 k^2 m n$.
\end{theorem}

Отметим, что для сверточных сетей зависимость от числа параметров слабее, чем для полносвязных сетей той же глубины, что объясняется параметризацией через свертки вместо полных матриц весов. Это согласуется с эмпирическими наблюдениями о том, что сверточные сети требуют меньше параметров для достижения того же качества.

\subsection{Влияние пулинга}

При добавлении операций пулинга (max pooling или average pooling) оценка спектральной нормы Гессе модифицируется. Для max pooling с ядром размера $k_{\text{pool}} \times k_{\text{pool}}$ оценка включает множитель $\left(\frac{1}{k_{\text{pool}}^2}\right)^{L-l+2}$, где $l$~--- номер слоя с пулингом~\cite{meshkov2024convnets}. Это показывает, что пулинг снижает кривизну поверхности потерь за счет уменьшения размерности представлений.

\section{Оценки спектральной нормы Гессе для трансформеров}

Для архитектур трансформеров оценка спектральной нормы Гессе учитывает взаимодействие компонентов: Self-Attention, Layer Normalization, Feed-Forward Network и residual connections~\cite{kiselev2024transformers}.

\subsection{Структура трансформера}

Стандартный блок трансформера с post-layer normalization состоит из следующих компонентов:
\begin{align}
\mathbf{Y} &= \text{LayerNorm}(\mathbf{X} + \text{SA}(\mathbf{X})), \label{eq:transformer-mid} \\
\mathbf{Z} &= \text{LayerNorm}(\mathbf{Y} + \text{FFN}(\mathbf{Y})), \label{eq:transformer-out}
\end{align}
где $\text{SA}(\mathbf{X}) = \mathbf{A}(\mathbf{X}) \mathbf{X} \mathbf{W}_V$~--- Self-Attention с матрицей внимания $\mathbf{A}(\mathbf{X}) = \text{softmax}(\mathbf{X} \mathbf{W}_Q \mathbf{W}_K^\top \mathbf{X}^\top / \sqrt{d_K})$, $\text{FFN}(\cdot)$~--- Feed-Forward Network, $\text{LayerNorm}(\cdot)$~--- нормализация по слою.

\subsection{Гессиан Self-Attention}

Для изолированного механизма Self-Attention Гессиан разлагается на outer-product и functional термы через разложение Гаусса"--~Ньютона. Спектральные нормы блоков Гессиана для различных пар весовых матриц ($\mathbf{W}_Q$, $\mathbf{W}_K$, $\mathbf{W}_V$) имеют различную зависимость от нормы входа $\|\mathbf{X}\|_2$:

\begin{theorem}\label{th:sa-hessian}
Рассмотрим одно-головый Self-Attention с $\text{SA}(\mathbf{X}) = \mathbf{A}(\mathbf{X}) \mathbf{X} \mathbf{W}_V$, где $\mathbf{A}(\mathbf{X}) = \text{softmax}(\mathbf{X} \mathbf{W}_Q \mathbf{W}_K^\top \mathbf{X}^\top / \sqrt{d_K})$, и произвольную дважды дифференцируемую функцию потерь $\mathcal{L}$. Пусть $\mathbf{H}^{(i,j)}$~--- $(i,j)$-блок Гессиана потерь относительно $\mathbf{W}_i, \mathbf{W}_j \in \{\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V\}$. Тогда каждый блок допускает разложение Гаусса"--~Ньютона $\mathbf{H}^{(i,j)} = \mathbf{H}_o^{(i,j)} + \mathbf{H}_f^{(i,j)}$, и его спектральная норма ограничена. Блоки масштабируются с нормой входа следующим образом:
\begin{equation}
\begin{array}{c|ccc}
 & Q & K & V \\ \hline
Q & \mathcal{O}(\|\mathbf{X}\|_2^6) & \mathcal{O}(\|\mathbf{X}\|_2^6) & \mathcal{O}(\|\mathbf{X}\|_2^5) \\
K & \cdot & \mathcal{O}(\|\mathbf{X}\|_2^6) & \mathcal{O}(\|\mathbf{X}\|_2^5) \\
V & \cdot & \cdot & \mathcal{O}(\|\mathbf{X}\|_2^4)
\end{array}
\end{equation}
причем ведущие функциональные термы затухают как $\mathcal{O}(L^{-3})$, где $L$~--- длина последовательности.
\end{theorem}

Эта оценка показывает крайнюю чувствительность кривизны Self-Attention к норме входа: доминирующие термы масштабируются как $\mathcal{O}(\|\mathbf{X}\|_2^6)$. Это объясняет важность pre-layer normalization для стабилизации кривизны, что согласуется с эмпирическими наблюдениями~\cite{xiong2020layer}.

\subsection{Гессиан Layer Normalization}

Layer Normalization вводит дополнительную нелинейность через нормализацию по строкам. Для входной матрицы $\mathbf{U} \in \mathbb{R}^{m \times n}$ операция LayerNorm определяется построчно:
\begin{equation}
\text{LayerNorm}(\mathbf{U})_{i,j} = \gamma_j \frac{\mathbf{U}_{i,j} - \mu_i}{\sqrt{\sigma_i^2}} + \beta_j,
\end{equation}
где $\mu_i = \frac{1}{n} \sum_{j=1}^n \mathbf{U}_{i,j}$, $\sigma_i^2 = \frac{1}{n} \sum_{j=1}^n (\mathbf{U}_{i,j} - \mu_i)^2$~--- среднее и дисперсия $i$-й строки.

Вычисление Гессиана LayerNorm требует учета производных операторов центрирования и нормализации дисперсии. Полученные выражения показывают, что кривизна, вносимая LayerNorm, зависит от нормы входа и параметров нормализации.

\subsection{Гессиан Feed-Forward Network}

Feed-Forward Network в трансформере состоит из двух линейных преобразований, разделенных нелинейной активацией (например, ReLU или GELU). Гессиан FFN вычисляется через разложение композиции линейных операций и активаций, аналогично полносвязным сетям, но с учетом контекста трансформера (вход~--- результат Self-Attention после LayerNorm).

\subsection{Полный Гессиан трансформера}

Объединяя Гессианы всех компонентов с учетом residual connections и композиции операций, получаем оценку спектральной нормы полного Гессиана блока трансформера. Эта оценка показывает, что кривизна определяется взаимодействием всех компонентов и зависит от архитектурных параметров: размерности модели $d_V$, размерности ключей $d_K$, числа слоев и длины последовательности $L$.

\section{Теорема о сходимости поверхности потерь}

Объединяя оценки спектральной нормы Гессе с разложением функции потерь, получаем теорему о сходимости поверхности потерь при увеличении размера выборки.

\begin{theorem}\label{th:landscape-convergence}
Пусть $\mathcal{L}_k(\mathbf{w})$~--- эмпирическая функция потерь по подвыборке размера $k$, $\mathbf{w}^*$~--- точка минимума, и существует окрестность $B_R(\mathbf{w}^*) = \{\mathbf{w} : \|\mathbf{w} - \mathbf{w}^*\|_2 \leqslant R\}$ такая, что спектральная норма матрицы Гессе ограничена: $\|\mathbf{H}_k(\mathbf{w})\|_2 \leqslant H_{\max}$ для всех $\mathbf{w} \in B_R(\mathbf{w}^*)$ и всех $k$. Предположим также, что функция потерь ограничена: $|\ell(f_{\mathbf{w}^*}(\mathbf{x}_i), \mathbf{y}_i)| \leqslant M_{\ell}$ для всех объектов $i$. Тогда для всех $\mathbf{w} \in B_R(\mathbf{w}^*)$ выполняется:
\begin{equation}
\label{eq:convergence-bound}
|\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})| \leqslant \frac{2}{k+1}\left(M_{\ell} + H_{\max} R^2\right) \to 0 \quad \text{при } k \to \infty.
\end{equation}
\end{theorem}

\begin{proof}
Используя разложение \eqref{eq:loss-difference-bound} и ограниченность функции потерь, получаем:
\[
|\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})| \leqslant \frac{1}{k+1}\left|\ell(f_{\mathbf{w}^*}(\mathbf{x}_{k+1}), \mathbf{y}_{k+1}) - \mathcal{L}_k(\mathbf{w}^*)\right| + \frac{1}{k+1}\|\mathbf{w} - \mathbf{w}^*\|_2^2 \left\|\mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k}\sum_{i=1}^k \mathbf{H}_i(\mathbf{w}^*)\right\|_2.
\]
Первое слагаемое ограничено $2M_{\ell}/(k+1)$ при ограниченности потерь. Второе слагаемое оценивается через спектральную норму Гессе:
\[
\left\|\mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k}\sum_{i=1}^k \mathbf{H}_i(\mathbf{w}^*)\right\|_2 \leqslant \|\mathbf{H}_{k+1}(\mathbf{w}^*)\|_2 + \frac{1}{k}\sum_{i=1}^k \|\mathbf{H}_i(\mathbf{w}^*)\|_2 \leqslant 2H_{\max},
\]
откуда следует оценка \eqref{eq:convergence-bound}.
\end{proof}

Теорема~\ref{th:landscape-convergence} показывает, что поверхность потерь стабилизируется со скоростью $O(1/k)$ при увеличении размера выборки. Это дает теоретическое обоснование использования сходимости поверхности как признака достаточного размера выборки: если для заданного порога $\varepsilon$ выполняется $2(M_{\ell} + H_{\max} R^2)/(k+1) \leqslant \varepsilon$, то выборка размера $k$ может считаться достаточной.

\section{Вычислительные эксперименты}

TODO: Описание экспериментов на различных архитектурах и датасетах, проверка теоретических оценок, сравнение с эмпирическими законами масштабирования.

\section{Обсуждение и связь с законами масштабирования}

Стабилизация поверхности потерь при увеличении объема выборки тесно связана с эмпирическими законами масштабирования~\cite{kaplan2020scaling,hoffmann2022chinchila}. Теоретические оценки, полученные в настоящей главе, дают обоснование этих законов через геометрию оптимизационной задачи.

Согласованное масштабирование модели и объема данных~\cite{hoffmann2022chinchila} обеспечивает, что поверхность потерь успевает стабилизироваться при росте сложности модели. Разработанные методы позволяют количественно оценить, при каком объеме выборки такая стабилизация достигается для заданной модели и порога $\varepsilon$.

Отметим, что подход, основанный на анализе поверхности потерь, единообразен для линейных и нейросетевых моделей: в обоих случаях стабилизация геометрии оптимизационной задачи служит признаком достаточности выборки. Для линейных моделей это выражается через сходимость апостериорных распределений (глава~\ref{ch:linear}), для нейросетей~--- через сходимость поверхности потерь и спектральные свойства Гессе (настоящая глава).

Полученные оценки спектральной нормы Гессе показывают зависимость от архитектуры сети:
\begin{itemize}
\item Для полносвязных сетей: $\|\mathbf{H}\|_2 \propto L (hM)^{2L}$~--- экспоненциальная зависимость от глубины и степенная от ширины.
\item Для сверточных сетей: $\|\mathbf{H}\|_2 \propto (C^2 k^2 w^2 m n)^L$~--- зависимость от числа каналов, размера ядра и размеров изображения.
\item Для трансформеров: кривизна определяется взаимодействием Self-Attention, LayerNorm и FFN, с доминирующей зависимостью от нормы входа как $\mathcal{O}(\|\mathbf{X}\|_2^6)$ для блоков внимания.
\end{itemize}

Эти зависимости объясняют, почему более глубокие сети требуют больше данных для стабилизации поверхности потерь, и почему архитектурные решения (например, pre-layer normalization в трансформерах) критически важны для стабильности оптимизации.