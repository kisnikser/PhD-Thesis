\chapter{Распределенный подход к оценке сходимости оптимизационной поверхности}\label{ch:robust}

В данной главе рассматриваются численные методы оценки сходимости оптимизационной поверхности при увеличении размера выборки. Анализ главы~\ref{ch:landscapes} был точечным: сходимость оценивалась в одной точке минимума через разложение Тейлора и спектральную норму матрицы Гессе. Здесь предлагается распределенный подход: вместо анализа в одной точке рассматривается математическое ожидание квадратичной разности потерь по распределению параметров в окрестности минимума. Это дает более робастную оценку стабилизации поверхности, поскольку характеризует поведение целых окрестностей решений, а не изолированных точек. Для практической реализации используется метод Монте"--~Карло с гауссовским сэмплированием. Для снижения вычислительной сложности предлагается переход к подпространству меньшей размерности, натянутому на главные собственные векторы матрицы Гессе. Получены теоретические оценки скорости сходимости распределенного подхода и показано, что она совпадает с точечным случаем, но дает более сильные гарантии стабильности.

\section{От точечного к распределенному анализу}

В главе~\ref{ch:landscapes} сходимость поверхности потерь анализировалась точечно: для точки минимума $\mathbf{w}^*$ оценивалась разность $|\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})|$ в окрестности $\mathbf{w}^*$ через разложение Тейлора второго порядка и спектральную норму матрицы Гессе. Этот подход дает теоретические оценки скорости сходимости $O(1/k)$, но имеет ограничения.

Во-первых, точечный анализ чувствителен к выбору конкретного минимума: в нейронных сетях существует множество эквивалентных минимумов, и поведение в одном из них может не отражать общую стабилизацию поверхности. Во-вторых, оценка в одной точке не характеризует стабилизацию окрестности минимума в целом: даже если разность потерь мала в точке $\mathbf{w}^*$, она может быть значительной в соседних точках. В-третьих, для практического применения требуется численная оценка сходимости, которую удобно получать усреднением по множеству точек.

Распределенный подход обобщает точечный анализ: вместо оценки в одной точке рассматривается математическое ожидание квадратичной разности потерь по распределению параметров $p(\mathbf{w})$ в окрестности минимума. Это позволяет охарактеризовать стабилизацию целых окрестностей решений и получить более робастную оценку достаточности выборки.

\section{Критерий квадратичной разности и распределенный подход}

Пусть $\mathbf{w}^*$~--- локальный минимум функций $\mathcal{L}_k(\mathbf{w})$ и $\mathcal{L}_{k+1}(\mathbf{w})$. Рассмотрим распределение параметров $p(\mathbf{w})$ в окрестности минимума (например, гауссовское распределение $p(\mathbf{w}) = \mathcal{N}(\mathbf{w}^*, \sigma^2 \mathbf{I})$).

\begin{definition}
Критерием квадратичной разности называется величина
\begin{equation}
\label{eq:delta-criterion}
\Delta_{k+1} = \mathbb{E}_{p(\mathbf{w})}\left[\left(\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})\right)^2\right],
\end{equation}
где математическое ожидание берется по распределению $p(\mathbf{w})$.
\end{definition}

Критерий $\Delta_{k+1}$ характеризует среднюю квадратичную разность потерь при добавлении $(k+1)$-го объекта, усредненную по окрестности минимума. При малом $\Delta_{k+1}$ поверхность потерь стабильна в окрестности минимума: добавление нового объекта почти не меняет значения функции потерь в среднем по распределению параметров.

Используя разложение дисперсии, критерий можно записать в виде:
\begin{equation}
\label{eq:delta-decomposition}
\Delta_{k+1} = \mathbb{D}_{p(\mathbf{w})}\left[\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})\right] + \left(\mathbb{E}_{p(\mathbf{w})}\left[\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w})\right]\right)^2,
\end{equation}
где первое слагаемое~--- дисперсия разности потерь, второе~--- квадрат математического ожидания.

Для гауссовского распределения $p(\mathbf{w}) = \mathcal{N}(\mathbf{w}^*, \sigma^2 \mathbf{I})$ и при естественном предположении, что $\mathcal{L}_{k+1}(\mathbf{w}^*) \approx \mathcal{L}_k(\mathbf{w}^*)$, математическое ожидание разности близко к нулю, и критерий определяется в основном дисперсией.

\section{Разложение через матрицу Гессе}

Используя разложение Тейлора второго порядка в окрестности минимума~$\mathbf{w}^*$, разность потерь можно выразить через разность матриц Гессе. Пусть
\begin{equation}
\label{eq:hessian-difference}
\mathbf{A}_k = \mathbf{H}^{(k+1)}(\mathbf{w}^*) - \mathbf{H}^{(k)}(\mathbf{w}^*),
\end{equation}
где $\mathbf{H}^{(k)}(\mathbf{w}^*) = \nabla^2_{\mathbf{w}} \mathcal{L}_k(\mathbf{w}^*)$~--- матрица Гессе функции потерь по подвыборке размера $k$.

При разложении Тейлора второго порядка разность потерь в точке $\mathbf{w} = \mathbf{w}^* + \boldsymbol{\delta}$ имеет вид:
\begin{equation}
\label{eq:loss-diff-taylor}
\mathcal{L}_{k+1}(\mathbf{w}) - \mathcal{L}_k(\mathbf{w}) \approx \mathcal{L}_{k+1}(\mathbf{w}^*) - \mathcal{L}_k(\mathbf{w}^*) + \frac{1}{2}\boldsymbol{\delta}^\top \mathbf{A}_k \boldsymbol{\delta}.
\end{equation}

Для гауссовского распределения $\boldsymbol{\delta} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$ и при условии $\mathcal{L}_{k+1}(\mathbf{w}^*) \approx \mathcal{L}_k(\mathbf{w}^*)$ критерий квадратичной разности можно выразить через следы матрицы $\mathbf{A}_k$:

\begin{lemma}\label{lem:delta-trace}
Пусть $p(\mathbf{w}) = \mathcal{N}(\mathbf{w}^*, \sigma^2 \mathbf{I})$ и $\mathcal{L}_{k+1}(\mathbf{w}^*) = \mathcal{L}_k(\mathbf{w}^*)$. Тогда критерий квадратичной разности выражается через следы матрицы $\mathbf{A}_k$:
\begin{equation}
\label{eq:delta-trace}
\Delta_{k+1} = \frac{\sigma^4}{4}\left(\mathrm{Tr}^2(\mathbf{A}_k) + 2\mathrm{Tr}(\mathbf{A}_k^2)\right).
\end{equation}
\end{lemma}

\begin{proof}
Используя разложение \eqref{eq:loss-diff-taylor} и свойства гауссовских моментов, получаем:
\[
\mathbb{E}\left[(\boldsymbol{\delta}^\top \mathbf{A}_k \boldsymbol{\delta})^2\right] = \sigma^4\left(\mathrm{Tr}^2(\mathbf{A}_k) + 2\mathrm{Tr}(\mathbf{A}_k^2)\right),
\]
откуда следует формула \eqref{eq:delta-trace}.
\end{proof}

\section{Оценка следа через спектральную норму}

Для практического применения леммы~\ref{lem:delta-trace} необходимо оценить следы матрицы $\mathbf{A}_k$ через более доступные характеристики, такие как спектральная норма.

\begin{lemma}\label{lem:trace-bound}
Для любой симметрической матрицы $\mathbf{A}_k \in \mathbb{R}^{N \times N}$ выполняется неравенство:
\begin{equation}
\label{eq:trace-bound}
\mathrm{Tr}^2(\mathbf{A}_k) + 2\mathrm{Tr}(\mathbf{A}_k^2) \leqslant N(N+2)\|\mathbf{A}_k\|_2^2,
\end{equation}
где $N$~--- размерность пространства параметров.
\end{lemma}

\begin{proof}
Пусть $\lambda_1, \ldots, \lambda_N$~--- собственные значения матрицы $\mathbf{A}_k$. Тогда
\[
\mathrm{Tr}(\mathbf{A}_k) = \sum_{i=1}^N \lambda_i, \qquad \mathrm{Tr}(\mathbf{A}_k^2) = \sum_{i=1}^N \lambda_i^2.
\]
По неравенству Коши"--~Буняковского:
\[
\left(\sum_{i=1}^N \lambda_i\right)^2 \leqslant N \sum_{i=1}^N \lambda_i^2 = N \mathrm{Tr}(\mathbf{A}_k^2).
\]
Следовательно,
\[
\mathrm{Tr}^2(\mathbf{A}_k) + 2\mathrm{Tr}(\mathbf{A}_k^2) \leqslant N \mathrm{Tr}(\mathbf{A}_k^2) + 2\mathrm{Tr}(\mathbf{A}_k^2) = (N+2)\mathrm{Tr}(\mathbf{A}_k^2).
\]
Поскольку $\mathrm{Tr}(\mathbf{A}_k^2) = \|\mathbf{A}_k\|_F^2 \leqslant N \|\mathbf{A}_k\|_2^2$, получаем требуемую оценку.
\end{proof}

\section{Оценка разности матриц Гессе}

Для применения лемм~\ref{lem:delta-trace} и~\ref{lem:trace-bound} необходимо оценить спектральную норму разности матриц Гессе $\|\mathbf{A}_k\|_2$. По определению,
\[
\mathbf{A}_k = \frac{1}{k+1}\left(\mathbf{H}_{k+1}(\mathbf{w}^*) - \frac{1}{k}\sum_{i=1}^k \mathbf{H}_i(\mathbf{w}^*)\right),
\]
где $\mathbf{H}_i(\mathbf{w}^*) = \nabla^2_{\mathbf{w}} \ell_i(\mathbf{w}^*)$~--- матрица Гессе потери на $i$-м объекте.

При условии ограниченности спектральных норм матриц Гессе: $\|\mathbf{H}_i(\mathbf{w}^*)\|_2 \leqslant M_{\mathbf{H}}$ для всех $i$, получаем оценку:
\begin{equation}
\label{eq:A-bound}
\|\mathbf{A}_k\|_2 \leqslant \frac{1}{k+1}\left(\|\mathbf{H}_{k+1}(\mathbf{w}^*)\|_2 + \frac{1}{k}\sum_{i=1}^k \|\mathbf{H}_i(\mathbf{w}^*)\|_2\right) \leqslant \frac{2M_{\mathbf{H}}}{k+1}.
\end{equation}

Эта оценка показывает, что разность матриц Гессе убывает как $O(1/k)$ при увеличении размера выборки, что согласуется с интуицией о стабилизации кривизны поверхности при добавлении данных.

\section{Теорема о скорости сходимости распределенного подхода}

Объединяя оценки из предыдущих разделов, получаем теорему о скорости сходимости критерия квадратичной разности.

\begin{theorem}\label{th:distributional-convergence}
Пусть выполнены следующие условия:
\begin{enumerate}
\item Функции потерь $\ell_i(\mathbf{w})$ дважды непрерывно дифференцируемы.
\item Существует локальный минимум $\mathbf{w}^*$ такой, что $\nabla \mathcal{L}_k(\mathbf{w}^*) = \nabla \mathcal{L}_{k+1}(\mathbf{w}^*) = \mathbf{0}$.
\item Спектральные нормы матриц Гессе ограничены: $\|\mathbf{H}_i(\mathbf{w}^*)\|_2 \leqslant M_{\mathbf{H}}$ для всех $i$.
\item Распределение параметров гауссовское: $p(\mathbf{w}) = \mathcal{N}(\mathbf{w}^*, \sigma^2 \mathbf{I})$.
\end{enumerate}
Тогда критерий квадратичной разности сходится со скоростью:
\begin{equation}
\label{eq:delta-convergence-rate}
\Delta_{k+1} = \mathcal{O}\left(\frac{1}{k^2}\right).
\end{equation}
\end{theorem}

\begin{proof}
Из леммы~\ref{lem:delta-trace} и оценки \eqref{eq:trace-bound} следует:
\[
\Delta_{k+1} \leqslant \frac{\sigma^4}{4} N(N+2) \|\mathbf{A}_k\|_2^2.
\]
Подставляя оценку \eqref{eq:A-bound}, получаем:
\[
\Delta_{k+1} \leqslant \frac{\sigma^4 N(N+2) M_{\mathbf{H}}^2}{(k+1)^2},
\]
что доказывает теорему.
\end{proof}

Теорема~\ref{th:distributional-convergence} показывает, что распределенный подход дает квадратичную скорость сходимости $O(1/k^2)$, что совпадает с асимптотической скоростью точечного подхода, но обеспечивает более сильные гарантии: стабилизация происходит не только в одной точке, но и в целой окрестности минимума.

\section{Монте"--~Карло оценка критерия квадратичной разности}

Для практического вычисления критерия $\Delta_{k+1}$ используется метод Монте"--~Карло. Математическое ожидание \eqref{eq:delta-criterion} аппроксимируется средним арифметическим по выборке точек из распределения $p(\mathbf{w})$:

\begin{equation}
\label{eq:mc-estimate}
\Delta_{k+1} \approx \frac{1}{B}\sum_{t=1}^B \left(\mathcal{L}_{k+1}(\mathbf{w}^{(t)}) - \mathcal{L}_k(\mathbf{w}^{(t)})\right)^2, \quad \mathbf{w}^{(t)} \sim p(\mathbf{w}),
\end{equation}
где $B$~--- число сэмплов Монте"--~Карло.

При гауссовском распределении $p(\mathbf{w}) = \mathcal{N}(\mathbf{w}^*, \sigma^2 \mathbf{I})$ точки сэмплируются как $\mathbf{w}^{(t)} = \mathbf{w}^* + \boldsymbol{\delta}^{(t)}$, где $\boldsymbol{\delta}^{(t)} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I})$. Параметр $\sigma$ определяет радиус окрестности минимума, в которой оценивается сходимость: при малом $\sigma$ анализ локальный, при большом~--- охватывает более широкую область.

Вычислительная сложность метода Монте"--~Карло линейна по числу сэмплов $B$ и размерности пространства параметров $N$. Для больших сетей с миллионами параметров вычисление функции потерь в каждой точке $\mathbf{w}^{(t)}$ может быть дорогим. Это мотивирует переход к подпространству меньшей размерности.

\section{Переход к подпространству главных собственных векторов Гессе}

Вычислительная сложность метода Монте"--~Карло растет с размерностью пространства параметров $N$, которая для больших сетей может достигать миллионов. Для снижения сложности предлагается переход к подпространству меньшей размерности, натянутому на главные собственные векторы матрицы Гессе~\cite{papyan2019spectrum}.

\subsection{Обоснование перехода к подпространству}

Эмпирические исследования показывают, что спектр матрицы Гессе в окрестности минимума имеет характерную структуру: множество малых собственных значений (bulk) и небольшое число выбросов (outliers)~\cite{papyan2019spectrum}. Это означает, что матрица Гессе имеет низкий эффективный ранг: только несколько направлений (соответствующих большим собственным значениям) вносят существенный вклад в изменение функции потерь.

Пусть $\mathbf{e}_1, \ldots, \mathbf{e}_D$~--- собственные векторы матрицы $\mathbf{H}(\mathbf{w}^*)$, соответствующие $D$ наибольшим по модулю собственным значениям $\lambda_1 \geqslant \lambda_2 \geqslant \ldots \geqslant \lambda_D$. Рассмотрим проекцию параметров на подпространство $\text{span}\{\mathbf{e}_1, \ldots, \mathbf{e}_D\}$:
\begin{equation}
\label{eq:projection}
\mathbf{w} = \mathbf{w}^* + \sum_{i=1}^D \theta_i \mathbf{e}_i = \mathbf{w}^* + \mathbf{P} \boldsymbol{\theta},
\end{equation}
где $\mathbf{P} = [\mathbf{e}_1, \ldots, \mathbf{e}_D] \in \mathbb{R}^{N \times D}$~--- матрица проекции, $\boldsymbol{\theta} = (\theta_1, \ldots, \theta_D)^\top \in \mathbb{R}^D$~--- координаты в подпространстве размерности $D \ll N$.

\subsection{Оценка критерия через собственные значения}

При проекции на подпространство главных собственных векторов разложение Тейлора второго порядка упрощается. Пусть $\boldsymbol{\Lambda}_k = \text{diag}(\lambda_k^{(1)}, \ldots, \lambda_k^{(D)})$~--- диагональная матрица из $D$ наибольших собственных значений матрицы Гессе $\mathbf{H}^{(k)}(\mathbf{w}^*)$.

\begin{theorem}\label{th:subspace-delta}
Пусть параметры проектируются на подпространство главных $D$ собственных векторов матриц Гессе, и $\boldsymbol{\theta} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_D)$. Тогда критерий квадратичной разности приближенно выражается через собственные значения:
\begin{equation}
\label{eq:delta-eigenvalues}
\Delta_k \approx \frac{\sigma^4}{4}\left(2\sum_{d=1}^D (\lambda_{k+1}^{(d)} - \lambda_k^{(d)})^2 + \left(\sum_{d=1}^D (\lambda_{k+1}^{(d)} - \lambda_k^{(d)})\right)^2\right),
\end{equation}
где $\lambda_k^{(d)}$ и $\lambda_{k+1}^{(d)}$~--- $d$-е наибольшие собственные значения матриц Гессе $\mathbf{H}^{(k)}(\mathbf{w}^*)$ и $\mathbf{H}^{(k+1)}(\mathbf{w}^*)$ соответственно.
\end{theorem}

\begin{proof}
При проекции на подпространство разложение Тейлора принимает вид:
\[
\mathcal{L}_k(\mathbf{w}^* + \mathbf{P}\boldsymbol{\theta}) \approx \mathcal{L}_k(\mathbf{w}^*) + \frac{1}{2}\boldsymbol{\theta}^\top \boldsymbol{\Lambda}_k \boldsymbol{\theta},
\]
где $\boldsymbol{\Lambda}_k = \mathbf{P}^\top \mathbf{H}^{(k)}(\mathbf{w}^*) \mathbf{P}$~--- диагональная матрица собственных значений. Разность потерь:
\[
\mathcal{L}_{k+1}(\mathbf{w}^* + \mathbf{P}\boldsymbol{\theta}) - \mathcal{L}_k(\mathbf{w}^* + \mathbf{P}\boldsymbol{\theta}) \approx \frac{1}{2}\boldsymbol{\theta}^\top (\boldsymbol{\Lambda}_{k+1} - \boldsymbol{\Lambda}_k) \boldsymbol{\theta}.
\]
Для гауссовского распределения $\boldsymbol{\theta} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_D)$ математическое ожидание квадрата этой разности выражается через следы матрицы $\boldsymbol{\Lambda}_{k+1} - \boldsymbol{\Lambda}_k$, что дает формулу \eqref{eq:delta-eigenvalues}.
\end{proof}

Теорема~\ref{th:subspace-delta} показывает, что критерий $\Delta_k$ можно эффективно вычислять, используя только $D$ наибольших собственных значений матрицы Гессе, что существенно снижает вычислительную сложность при $D \ll N$.

\section{Сравнение точечного и распределенного подходов}

Распределенный подход обобщает точечный анализ главы~\ref{ch:landscapes} и имеет следующие преимущества:

\begin{enumerate}
\item \textbf{Робастность к выбору минимума}: усреднение по распределению параметров делает оценку менее чувствительной к конкретному выбору минимума среди эквивалентных решений.

\item \textbf{Характеризация окрестностей}: распределенный подход характеризует стабилизацию целых окрестностей минимума, а не только изолированных точек, что дает более полную картину сходимости поверхности.

\item \textbf{Практическая реализуемость}: метод Монте"--~Карло позволяет численно оценить критерий $\Delta_k$ без аналитического вычисления следов матриц, что делает подход применимым к сложным архитектурам.

\item \textbf{Вычислительная эффективность}: переход к подпространству главных собственных векторов существенно снижает вычислительную сложность при сохранении точности оценок.
\end{enumerate}

В то же время распределенный подход сохраняет асимптотическую скорость сходимости точечного случая $O(1/k^2)$, что показывает согласованность обоих подходов.

\section{Вычислительные эксперименты}

TODO: Описание экспериментов с различными архитектурами, проверка теоретических оценок, сравнение точечного и распределенного подходов, анализ влияния архитектурных выборов (batch normalization, dropout, глубина сети).

\section{Обсуждение}

Распределенный подход к оценке сходимости оптимизационной поверхности дает более робастную характеристику достаточности выборки по сравнению с точечным анализом. Критерий квадратичной разности $\Delta_k$ формализует геометрическое понятие достаточности: когда $\Delta_k$ падает ниже заданного порога, поверхность потерь становится достаточно стабильной, чтобы дополнительные образцы незначительно изменяли геометрию оптимизации.

Переход к подпространству главных собственных векторов Гессе позволяет эффективно вычислять критерий даже для больших сетей, используя только информацию о доминирующих направлениях кривизны. Это согласуется с эмпирическими наблюдениями о низком эффективном ранге матрицы Гессе в окрестности минимума.

Полученные результаты связывают геометрию оптимизационной поверхности с объемом данных, давая теоретическое обоснование эмпирических законов масштабирования и предоставляя практические инструменты для оценки достаточного размера выборки в нейросетевых моделях.
